This repository complements our survey on developments in sign language technology with a curated list of papers investigating sign language processing, including recognition, translation, and generation.

# Menu
- [Surveys](#surveys)
- [Papers](#papers)

# Surveys
## Sign language recognition
- **Automatic Sign Language Analysis: A Survey and the Future beyond Lexical Meaning**[TPAMI 2005] [[paper]](https://www.researchgate.net/publication/7799837_Automatic_Sign_Language_Analysis_A_Survey_and_the_Future_beyond_Lexical_Meaning)  
  **Authors:** Sylvie Ong, Surendra Ranganath  
  **Introduction:** This paper summarizes the research progress in the field of automatic sign language analysis, focusing on continuous sign language gesture recognition, non-gesture signal analysis and its combination with gestures, and proposes future research directions.

- **Image-based and sensor-based approaches to Arabic sign language recognition**[THMS 2014] [[paper]](https://ieeexplore.ieee.org/abstract/document/6814287)  
  **Authors:** Mohamed Mohandes, Mohamed Deriche, Junzhao Liu  
  **Introduction:** This paper reviews the methods and systems for automatic Arabic sign language recognition, analyzes the main challenges, and proposes future research directions.

- **Wearable sensor-based sign language recognition: A comprehensive review**[RBME 2020] [[paper]](https://ieeexplore.ieee.org/abstract/document/9178440)  
  **Authors:** Karly Kudrinko, Emile Flavin, Xiaodan Zhu, Qingguo Li  
  **Introduction:** This paper reviews the research on sign language recognition based on wearable sensors, reviews 72 studies from 1991–2019, analyzes trends, methods, and challenges, and provides a reference for the development of user-centered sign language recognition systems.

 - **A Comprehensive Study on Deep Learning-Based Methods for Sign Language Recognition**[TMM 2021] [[paper]](https://ieeexplore.ieee.org/abstract/document/9393618)  
  **Authors:** Nikolas Adaloglou, Theocharis Chatzis, Ilias Papastratis, Andreas Stergioulas, Georgios Th. Papadopoulos, Vassia Zacharopoulou, George J. Xydopoulos, Klimnis Atzakas, Dimitris Papazachariou, Petros Daras  
  **Introduction:** This paper reviews and evaluates deep learning-based sign language recognition methods, proposes a new sequence training criterion, discusses pre-training schemes, and creates the first multi-level annotation-level RGB+D dataset of Greek sign language.

- **Sign language recognition: A deep survey**[ESA 2021] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S095741742030614X)  
  **Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera  
  **Introduction:** This paper reviews the deep learning-based visual sign language recognition models developed in the past five years, and analyzes the classification, datasets, applications, challenges, and future research directions of isolated and continuous sign language recognition.

- **Artificial intelligence technologies for sign language**[Sensors 2021] [[paper]](https://www.mdpi.com/1424-8220/21/17/5843)  
  **Authors:** Ilias Papastratis, Christos Chatzikonstantinou, Dimitrios Konstantinidis, Kosmas Dimitropoulos, Petros Daras  
  **Introduction:** This paper reviews the deep learning-based visual sign language recognition models developed in the past five years, and analyzes the classification, datasets, applications, challenges, and future research directions of isolated and continuous sign language recognition.

- **A Survey of Sign Language Recognition, Translation, and Generation**[CS (China) 2021] [[paper]](https://qikan.cqvip.com/Qikan/Article/Detail?id=7103984847&from=Qikan_Article_Detail)  
  **Authors:** Dan Guo, Shengeng Tang, Richang Hong, Meng Wang  
  **Introduction:** This paper reviews the latest progress, typical methods and challenges in sign language recognition, translation and generation, and looks forward to the future development direction of this field.

- **Sign language recognition systems: A decade systematic literature review**[ACME 2021] [[paper]](https://link.springer.com/article/10.1007/s11831-019-09384-2)
        
        
        
        
  **Authors:** Dan Guo, Shengeng Tang, Richang Hong, Meng Wang  
  **Introduction:** This paper reviews the latest progress, typical methods and challenges in sign language recognition, translation and generation, and looks forward to the future development direction of this field.

- **Emerging wearable interfaces and algorithms for hand gesture recognition: A survey**[RBME 2021] [[paper]](https://ieeexplore.ieee.org/abstract/document/9426433)  
  **Authors:** Shuo Jiang, Peiqi Kang, Xinyu Song, Benny PL Lo, Peter B Shull  
  **Introduction:** This paper reviews the applications, method classification and challenges of wearable gesture interfaces and algorithms in gesture recognition, and proposes future research directions to improve accuracy and practicality.

- **A survey on sign language literature**[MLwA 2023] [[paper]](https://www.sciencedirect.com/science/article/pii/S2666827023000579)  
  **Authors:** Marie Alaghband, Hamid Reza Maghroor, Ivan Garibay  
  **Introduction:** This article reviews the research progress of sign language recognition and translation, covering methods, datasets, and applications, to provide a reference for bridging the communication gap between the hearing-impaired and the hearing-normal.

- **Unraveling a decade: A comprehensive survey on isolated sign language recognition**[CVPR 2023] [[paper]](https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Sarhan_Unraveling_a_Decade_A_Comprehensive_Survey_on_Isolated_Sign_Language_ICCVW_2023_paper.html)  
  **Authors:** Noha Sarhan, Simone Frintrop  
  **Introduction:** This paper reviews the latest methods, datasets, and performance of isolated sign language recognition (ISLR), and analyzes the research progress, challenges, and future development directions.

- **A Review of Sign Language Systems**[DeSE 2023] [[paper]](https://www.researchgate.net/publication/379160316_A_Review_of_Sign_Language_Systems)  
  **Authors:** Marzieh Moradi, Deepika Dhanabalan Kannan, Shiva Asadianfam, Hoshang Kolivand, Omar Aldhaibani  
  **Introduction:** Provides a comprehensive review of sign language systems, with particular attention to research on British Sign Language (BSL), and examines the legal, social, and ethical considerations associated with the use of sign languages.

- **Understanding Sign Language Recognition: An Overview**[ICEECT 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10739257)  
  **Authors:** Shivani Saini, Meenu Prjapati, Bhupal Arya, Ravinder Kumar Sharma, Manoj Kumar  
  **Introduction:** This paper reviews the progress of artificial intelligence technologies in the field of sign language recognition (SLR), covering both vision-based and sensory glove-based methods, aiming to help eliminate communication barriers for the deaf and mute.

- **A review of modular continuous sign language recognition algorithms and technologies**[Mini-Micro S. 2024] [[paper]](https://qikan.cqvip.com/Qikan/Article/Detail?id=7113060967&from=Qikan_Search_Index)  
  **Authors:** Meng Jinkai, Peng Jianjun, Xiao Zhidong, Guo Li, Jin Kai, Zheng Tong  
  **Introduction:** This paper reviews the research progress of continuous sign language recognition, focusing on the keyframe extraction, feature extraction and sequence learning modules in unimodal and multimodal frameworks, summarizes the datasets and evaluation metrics, and discusses the challenges of existing algorithms and future development directions.

- **A Survey of Methods and Technologies for Chinese Sign Language Recognition**[Mod. Spec. Educ. 2024] [[paper]](https://qikan.cqvip.com/Qikan/Article/Detail?id=7111887169&from=Qikan_Search_Index)  
  **Authors:** Meng Jinkai, Peng Jianjun, Xiao Zhidong, Guo Li, Jin Kai, Zheng Tong  
  **Introduction:** This paper reviews the development of Chinese sign language recognition from traditional methods to modern deep learning and artificial intelligence technologies, which provides important support for the communication between the hearing-impaired and the society.
   
- **Sign Language Recognition and Translation Systems for Enhanced Communication for the Hearing Impaired**[2024 1st IC-CGU] [[paper]](https://ieeexplore.ieee.org/abstract/document/10530832)  
  **Authors:** Kambhampati Sai Sindhu, Nikitha Biradar, Penumathsa Likhita Varma, Chandrasekhar Uddagiri, et al.  
  **Introduction:** This paper reviews the technical progress and challenges of sign language recognition and translation systems, emphasizing the importance of their grammatical complexity and diverse data requirements for developing efficient and inclusive communication tools.

- **Literature Review: Recognization of Sign Language and Translation Systems**[INSPECT 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10896036)  
  **Authors:** P. Jyothi, Rishitha Sagar Mogili, Raman Garg, Tanmayee Etla, R. Kundan  
  **Introduction:** This paper reviews the research progress of sign language recognition and translation, and analyzes key methods, technologies, and models to promote barrier-free communication between sign language users and non-sign language users.

- **Deep Learning Method for Sign Language Recognition: A Systematic Literature Review**[2024 ICIMTech] [[paper]](https://ieeexplore.ieee.org/abstract/document/10780830)  
  **Authors:** Sekar Ayu Nadita, Lavender Nathania Adelya, Daniel Hendra Susanto, Gusti Pangestu  
  **Introduction:** This paper systematically reviews the application of artificial intelligence (especially deep learning) in sign language recognition, evaluates the performance of different models, techniques and datasets, points out the challenges in research, and demonstrates the recognition accuracy of up to 99.98% and the potential for future development.

- **A review of real-time sign language recognition for virtual interaction on meeting platforms**[Confluence 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10463439)  
  **Authors:** Mohd Faisal, Angad Singh, Shailendra Narayan Singh  
  **Introduction:** This study reviews the current status, challenges, and technical methods of real-time sign language recognition and translation systems in virtual meetings, providing a reference for improving multilingual sign language-assisted communication.

- **Exploring Modern Approaches of Recognizing Sign Language: A Review**[ICAC²N 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10895826)  
  **Authors:** Rani Astya, Kalpana Mishra, Anil Kumar Sagar  
  **Introduction:** This paper reviews the latest machine learning methods and their applications for sign language recognition, analyzes the current technical advantages and challenges, and points out future research directions to improve system performance and usability.

- **A Survey on Machine and Deep Learning Approaches in Sign Language Recognition: Techniques and Future Trends**[IConSCEPT 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10627910)  
  **Authors:** C. Harshitha, Narendran Rajagopalan, et al.  
  **Introduction:** This review summarizes the progress in the application of machine learning and deep learning in sign language recognition and explores future research directions.

- **Advancements in sign language recognition: A comprehensive review and future prospects**[Access 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10670380)  
  **Authors:** Bashaer Al Abdullah, Ghada Amoudi, Hanan Alghamdi  
  **Introduction:** This paper systematically reviews the latest progress of automatic sign language translation systems (SLTS), emphasizes the role of deep learning in improving sign language recognition accuracy and integrating non-artificial features, and proposes future research directions to improve the applicability and performance of the systems.

- **VR Applications and Vision-Based Recognition Techniques for Sign Language: A Survey**[CICN 2024] [[paper]](https://ieeexplore.ieee.org/document/10847543)  
  **Authors:** Sumayyah Seher, Kulsoom Abedi, Kulsum Fatima, Sadiya Badiwalla, Taha Houda  
  **Introduction:** This article reviews the combination of virtual reality and visual sign language recognition, emphasizing its application potential and future development directions in immersive education and barrier-free sign language learning.

- **Sign Language or Gesture-Based Recognition System: A Review**[ASSIC 2024] [[paper]](https://ieeexplore.ieee.org/document/10507916)  
  **Authors:** Anjana Mishra, Ayush Kumar, Abhinav Kumar, Abhishek Mahato, Keshav Kamal  
  **Introduction:** This paper reviews the latest methods and challenges of sign language recognition using computer vision, machine learning, and Internet of Things technologies to facilitate efficient communication between deaf and mute people and others.

- **A review of deep learning-based approaches to sign language processing**[AR 2024] [[paper]](https://www.tandfonline.com/doi/full/10.1080/01691864.2024.2442721?src=)  
  **Authors:** Sihan Tan, Nabeela Khan, Zhaoyi An, Yoshitaka Ando, Rei Kawakami, Kazuhiro Nakadai  
  **Introduction:** This review systematically reviews the development of sign language recognition, translation, and generation technologies, focusing on the application of large language models, the applicability of datasets, and the limitations of existing methods, providing a reference for promoting standardized evaluation and the development of robust sign language processing systems.

- **Sign language recognition: A comprehensive review of traditional and deep learning approaches, datasets, and challenges**[Access 2024] [[paper]](https://ieeexplore.ieee.org/document/10526274)  
  **Authors:** Tangfei Tao, Yizhe Zhao, Tianyu Liu, Jieli Zhu  
  **Introduction:** This paper reviews the recent developments in sign language recognition (SLR), covering traditional and deep learning methods, datasets, challenges, and future directions, aiming to provide a reference for sign language understanding and research.

- **A Meta-Analytic Quantitative Review of Methods and Techniques for Sign Language Recognition Approaches**[IST-Africa 2024] [[paper]](https://ieeexplore.ieee.org/document/10569745)  
  **Authors:** Tebatso Gorgina Moape, Absolom Muzambi, Bester Chimbo  
  **Introduction:** This paper summarizes the overall effects and trends of machine learning and deep learning methods in the field of sign language recognition through meta-analysis, reveals significant heterogeneity among studies and fills the gap in systematic review.

- **A Survey of Dynamic Sign Language Recognition**[CSA 2025] [[paper]](https://www.c-s-a.org.cn/html/2025/5/9879.htm)  
  **Authors:** Wang Zhekai, Feng Yunxia, Wang Jiawen  
  **Introduction:** This paper reviews the current development status, methods, datasets and evaluation indicators of dynamic sign language recognition technology based on deep learning, analyzes the challenges and shortcomings of existing methods, and looks forward to future research directions.

- **Deep learning pathways for automatic sign language processing**[PR 2025] [[paper]](https://www.sciencedirect.com/science/article/pii/S0031320325001359)  
  **Authors:** Mukhiddin Toshpulatov, Wookey Lee, Jaesung Jun, Suan Lee  
  **Introduction:** This paper reviews the research progress in recognition, translation, generation and datasets in the field of sign language processing, analyzes key technologies and challenges, and proposes future development directions.

- **A Survey of Sign Language Recognition and Translation Based on Computer Vision**[M&C 2025] [[paper]](https://lib.cqvip.com/Qikan/Article/Detail?id=7201183340&from=Qikan_Search_Index)  
  **Authors:** Yunan Li, Xi Geng, Qiguang Miao  
  **Introduction:** This paper systematically reviews computer vision-based sign language recognition and translation methods, datasets, and evaluation metrics, analyzes their respective characteristics and challenges, and proposes future research directions.

- **Deep learning approaches for continuous sign language recognition: A comprehensive review**[Access 2025] [[paper]](https://ieeexplore.ieee.org/document/10937713)  
  **Authors:** Asma Khan, Seyong Jin, Geon-Hee Lee, Gul E Arzu, Tan N Nguyen, L Minh Dang, Woong Choi, Hyeonjoon Moon  
  **Introduction:** This paper reviews deep learning methods in the field of continuous sign language recognition (CSLR), proposes a unified classification framework, analyzes the advantages and disadvantages of spatial, temporal, and alignment strategies, and discusses key challenges such as dataset diversity, real-time processing, and differences among sign language users, providing guidance for future research.
   
## Sign language translation
- **Automatic Sign Language Analysis: A Survey and the Future beyond Lexical Meaning**[22nd NETA 2018] [[paper]](https://d.wanfangdata.com.cn/conference/10402801)  
  **Authors:** Ankita Wadhawan, Parteek Kumar  
  **Introduction:** This paper systematically reviews and categorizes sign language recognition research from 2007 to 2017, analyzing methods, gesture types, and performance to provide reference and guidance for future research.

- **Advances in machine translation for sign language: approaches, limitations, and challenges**[NCA 2021] [[paper]](https://link.springer.com/article/10.1007/s00521-021-06079-3)
        
        
        
        
  **Authors:** Uzma Farooq, Mohd Shafry Mohd Rahim, Nabeel Sabir, Amir Hussain, Adnan Abid  
  **Introduction:** This article reviews the latest research progress in sign language interpretation from a multidisciplinary perspective, classifies and analyzes methods, compares theoretical foundations, and explores challenges and future development directions.

- **Deep learning methods for sign language translation**[TACCESS 2021] [[paper]](https://dl.acm.org/doi/abs/10.1145/3477498)
        
        
        
        
  **Authors:** Tejaswini Ananthanarayana, Priyanshu Srivastava, Akash Chintha, Akhil Santha, Brian Landy, Joseph Panaro, Andre Webster, Nikunj Kotecha, Shagan Sah, Thomastine Sarchet, et al.  
  **Introduction:** This paper reviews and evaluates sign language translation methods based on deep learning, compares the performance of different input features and neural translation models on various sign language datasets, and points out that Transformer combined with ResNet50 or posture features performs best.

- **A survey of advancements in real-time sign language translators: integration with IoT technology**[Technologies 2023] [[paper]](https://www.mdpi.com/2227-7080/11/4/83)  
  **Authors:** Maria Papatsimouli, Panos Sarigiannidis, George F. Fragulis  
  **Introduction:** This article reviews the development of real-time sign language interpretation systems and their integration with Internet of Things technologies over the past five years, analyzing technological progress, current applications, and future potential to promote communication and inclusion for the deaf and hard-of-hearing community.

- **Sign language translation: A survey of approaches and techniques**[Electronics 2023] [[paper]](https://www.mdpi.com/2079-9292/12/12/2678)  
  **Authors:** Zeyu Liang, Huailing Li, Jianping Chai  
  **Introduction:** This paper reviews the research progress of sign language translation (SLT), including the basic model, Transformer framework, four types of subtasks, challenges and future directions.

- **A survey on Sign Language machine translation**[ESA 2023] [[paper]](https://www.sciencedirect.com/science/article/pii/S0957417422020115)  
  **Authors:** Adrián Núñez-Marcos, Olatz Perez-de-Viñaspre, Gorka Labaka  
  **Introduction:** This study systematically reviews the development of sign language translation (SLT), introduces the background and mainstream datasets, and summarizes the challenges and future research directions.

- **Machine translation from text to sign language: a systematic review**[UAIS 2023] [[paper]](https://link.springer.com/article/10.1007/s10209-021-00823-1)
        
        
        
        
  **Authors:** Navroz Kaur Kahlon, Williamjeet Singh  
  **Introduction:** This paper reviews the research progress, method classification, advantages and disadvantages of sign language machine translation and generation, and emphasizes the improvement of automated sign language translation through deep learning and neural networks.

- **Sign Language Recognition and Translation Methods Promote Sign Language Education: A Review**[SMC 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10831194)  
  **Authors:** Jingchen Zou, Jianqiang Li, Jing Tang, Yuning Huang, Shujie Ding, Xi Xu  
  **Introduction:** This paper systematically reviews the development, key technical methods and latest progress of sign language recognition and translation (SLRT), analyzes the limitations of existing methods and proposes future research directions.

- **A Survey on Recognition and Translation System of Real-Time Sign Language**[2024 2nd IDICAIEI] [[paper]](https://ieeexplore.ieee.org/abstract/document/10842738)  
  **Authors:** Kajal Dakhare, Vidhi Wankhede, Prateek Verma  
  **Introduction:** This paper reviews the latest deep learning methods for real-time sign language recognition systems and their applications in improving sign language understanding accuracy and real-time interaction, providing technical support for improving communication for the deaf and mute.

- **Deep Learning Methods of Dynamic Sign Language Translation: A Rapid Umbrella Review**[ICECET 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10698113)  
  **Authors:** Lebogang Puree Bopape, Mohohlo Samuel Tsoeu  
  **Introduction:** This paper presents the first systematic review of existing sign language translation and recognition, analyzing technical methods, geographical background, and linguistic characteristics. It also points out that existing research tends to be data-driven and lacks attention to the complexity of sign language and reasonable evaluation.

- **Review of Intelligent Methods for Sign Language-to-Speech Translation**[ICALTER 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10819213)  
  **Authors:** Edita Chafloque Cajusol Trujillo, Luis Felipe Castillo, Juri Aquino  
  **Introduction:** This paper reviews CNN- and RNN-based sign language-to-speech intelligent systems, analyzes their technical progress and challenges, and emphasizes their potential in enhancing the social inclusion and autonomy of the deaf and mute.

- **Deep Learning--Based Sign Language Translation: Past, Present, and Future**[ARC 2024] [[paper]](https://www.arocmag.cn/abs/2025.01.0001)  
  **Authors:** Lei Zhang, Zhenyu Wang, Shuaishuai Lian, Bingqian Pu, Yutao Liu, Mingzhe Qin  
  **Introduction:** This paper reviews deep learning-based sign language translation (SLT) methods, analyzes the characteristics and performance of different model categories, and explores future development directions such as real-time translation and large-scale model fine-tuning.

## Sign language production
- **Sign language production: A review**[CVPR 2021] [[paper]](https://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Rastgoo_Sign_Language_Production_A_Review_CVPRW_2021_paper.html)  
  **Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera, Mohammad Sabokrou  
  **Introduction:** This paper reviews the latest progress of deep learning in the field of sign language generation, and analyzes its advantages, limitations and future research directions.

- **A Survey on Neural Machine Translation Applied to Sign Language Generation**[ICAML 2021] [[paper]](https://ieeexplore.ieee.org/abstract/document/9712072)  
  **Authors:** Yue Zhang, Lihong Cao  
  **Introduction:** This paper reviews sign language generation methods based on neural machine translation, including RNN and Transformer models, datasets and evaluation methods, and proposes future research directions.

- **State of the art of automation in Sign Language: a systematic review**[TALLIP 2023] [[paper]](https://dl.acm.org/doi/abs/10.1145/3564769)  
  **Authors:** Rakesh Kumar Attar, Vishal Goyal, Lalit Goyal  
  **Introduction:** This paper systematically reviews the research progress, method classification and comparative analysis of sign language generation systems from 1998 to 2020, and proposes future research directions to guide the development of this field.

- **Development Status and Trends of Sign Language Digital Humans Based on Intelligent Generative Technologies**[AI 2023] [[paper]](https://qikan.cqvip.com/Qikan/Article/Detail?id=7110318684)  
  **Authors:** Shengeng Tang, Xueyu Xiu, Dan Guo, Richang Hong  
  **Introduction:** This paper reviews the current development status, challenges and future trends of sign language digital humans based on intelligent generation technology.

  - **Techniques for Generating Sign Language a Comprehensive Review**[JIEI:B 2024] [[paper]](https://link.springer.com/article/10.1007/s40031-024-01118-8)  
  **Authors:** Prachi Pramod Waghmare  
  **Introduction:** This article reviews the applications of deep learning and natural language processing techniques in sign language generation, highlighting the progress in automatically converting written or spoken language into sign language gestures, the importance of datasets and context, as well as the challenges and opportunities of real-time generation, diverse sign language variants, and ethical issues.

- **A survey on recent advances in Sign Language Production**[ESA 2024] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417423033481)  
  **Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera, Vassilis Athitsos, Mohammad Sabokrou  
  **Introduction:** This paper reviews the progress of sign language recognition and generation (SLP) technology based on deep learning, and discusses the key components, challenges, methodological framework and future research directions of bidirectional sign language translation systems.

# Papers
## Perception Modeling
- **Segmentation of the face and hands in sign language video sequences using color and motion cues**[TCSVT 2004] [[paper]](https://ieeexplore.ieee.org/document/1318645)  
  **Authors:** Nariman Habili, Cheng Chew Lim, Alireza Moini  
  **Introduction:** This paper proposes a hand-face segmentation method based on color and motion information. Through skin color segmentation, motion change detection and segmentation mask generation, it can achieve accurate positioning of hands and faces in sign language videos.

- **Accurate and Accessible Motion-Capture Glove Calibration for Sign Language Data Collection**[TACCESS 2010] [[paper]](https://dl.acm.org/doi/abs/10.1145/1838562.1838564)  
  **Authors:** Matt Huenerfauth, Pengfei Lu  
  **Introduction:** This paper proposes an efficient calibration scheme for motion capture gloves, which significantly improves calibration accuracy and ease of use. Experimental verification shows that it can better support American Sign Language gesture recording and animation generation.

- **Novel FPGA implementation of hand sign recognition system with SOM--Hebb classifier**[TCSVT 2014] [[paper]](https://ieeexplore.ieee.org/document/6848809)  
  **Authors:** Hiroomi Hikawa, Keishi Kaida  
  **Introduction:** This paper proposes an FPGA hardware gesture recognition system based on a SOM-Hebbian hybrid network to achieve efficient and robust recognition of 24 American Sign Language gestures, suitable for embedded applications.

- **Feature extraction in Brazilian Sign Language Recognition based on phonological structure and using RGB-D sensors**[ESA 2014] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417414003042)  
  **Authors:** Sí­lvia Grasiella Moreira Almeida, Frederico Gadelha Guimarães, Jaime Arturo Ramírez  
  **Introduction:** This paper proposes a feature extraction and classification method for Brazilian Sign Language (BSL) based on RGB-D features and the phonological structure of sign language. SVM is used to achieve gesture recognition with an average accuracy of over 80%.

- **Glove-based continuous Arabic sign language recognition in user-dependent mode**[THMS 2015] [[paper]](https://ieeexplore.ieee.org/document/7061411)  
  **Authors:** Noor Tubaiz, Tamer Shanableh, Khaled Assaleh  
  **Introduction:** This paper proposes a glove-based Arabic sign language recognition system that uses sensors to capture gestures and classifies them through an improved k-nearest neighbor method, achieving a sentence recognition rate of up to 98.9%.

- **Isolated sign language recognition with grassmann covariance matrices**[TACCESS 2016] [[paper]](https://dl.acm.org/doi/10.1145/2897735)  
  **Authors:** Hanjie Wang, Xiujuan Chai, Xiaopeng Hong, Guoying Zhao, Xilin Chen  
  **Introduction:** This paper proposes a multimodal sign language representation method based on the Grassmann covariance matrix, which implements discriminative learning through SVM. It outperforms existing methods on three datasets and achieves high accuracy and low computational cost.

- **A feature covariance matrix with serial particle filter for isolated sign language recognition**[ESA 2016] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417416300112)  
  **Authors:** Kian Ming Lim, Alan W.C. Tan, Shing Chiang Tan  
  **Introduction:** This paper proposes an isolated sign language recognition method based on feature covariance matrix and serial particle filtering. By fusing median and mode filtering, hand detection and tracking are achieved, laying the foundation for compact sign language representation and intelligent translation systems.

- **Hand sign language recognition using multi-view hand skeleton**[ESA 2020] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417420301615)  
  **Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera  
  **Introduction:** This paper proposes a multimodal pipeline combining SSD, 2DCNN, 3DCNN and LSTM for efficient sign language gesture recognition from RGB videos and constructing hand skeletons and discriminative spatiotemporal features.

- **ASL trigger recognition in mixed activity/signing sequences for RF sensor-based user interfaces**[THMS 2021] [[paper]](https://ieeexplore.ieee.org/document/9660776)  
  **Authors:** Emre Kurtoğlu, Ali C. Gurbuz, Evie A. Malaia, Darrin Griffin, Chris Crawford, Sevgi Z. Gurbuz  
  **Introduction:** This paper proposes a gesture recognition method based on radio frequency sensors. By using multi-domain data representation, it can distinguish trigger gestures from gross movements, achieving high accuracy rates of 98.9% and 92% for ASL words and gross movements, respectively.

- **Smart, comfortable wearable system for recognizing Arabic Sign Language in real-time using IMUs and features-based fusion**[ESA 2021] [[paper]](https://ieeexplore.ieee.org/document/9660776)  
  **Authors:** Aziz Qaroush, Sara Yassin, Ali Al-Nubani, Ameer Alqam  
  **Introduction:** This paper proposes an Arabic sign language recognition system based on a multi-IMU sensor glove. By fusing accelerometer and gyroscope features, it realizes letter-level gesture recognition, achieving user-dependent and independent recognition accuracies of 98.6% and 96%, respectively.
  
- **Double handed dynamic Turkish Sign Language recognition using Leap Motion with meta learning approach**[ESA 2023] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417423009557)  
  **Authors:** Zekeriya Katılmiş, Cihan Karakuzu  
  **Introduction:** This paper studies bimanual dynamic word recognition in Turkish Sign Language using the LMC device, achieving high-performance and stable word recognition through three-stage feature processing and Meta-ELM classifier.

- **Comparison between handcraft feature extraction and methods based on Recurrent Neural Network models for gesture recognition by instrumented gloves: A case for Brazilian Sign Language Alphabet**[BSPC 2023] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S1746809422006553)  
  **Authors:** Thiago Simões Dias, José Jair Alves Mendes Junior, Sérgio Francisco Pichorim  
  **Introduction:** This paper reviews sensor- and deep learning-based Brazilian Sign Language (Libras) recognition methods and proposes an integrated GRU model to classify signals collected by an instrumented glove to compare the performance with and without manual feature extraction methods.

- **Static hand gesture recognition in sign language based on convolutional neural network with feature extraction method using ORB descriptor and Gabor filter**[ESA 2023] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417422016256)  
  **Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera  
  **Introduction:** This paper proposes a deep learning sign language static gesture recognition network that combines CNN, Gabor filter and ORB features to achieve high accuracy and enhance robustness to uncertainties such as rotation and blur.

- **Spatial-temporal feature-based End-to-end Fourier network for 3D sign language recognition**[ESA 2024] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417424001234)  
  **Authors:** Sunusi Bala Abdullahi, Kosin Chamnongthai, Veronica Bolon-Canedo, Brais Cancela  
  **Introduction:** This paper proposes a dynamic sign language word recognition method based on multi-scale spatiotemporal features (FS-EFCNN), which effectively improves the recognition accuracy of different sign language datasets through low-cost feature selection and Fourier convolutional neural network.
  
## Sign language recognition
### Statistical model-driven temporal modeling
- **Visual recognition of american sign language using hidden markov models**[MIT 1995] [[paper]](https://www.researchgate.net/publication/33835956_Visual_Recognition_of_American_Sign_Language_Using_Hidden_Markov_Models)  
  **Authors:** Starner, Thad  
  **Introduction:** This paper proposes a sentence-level American Sign Language recognition system based on Hidden Markov Models, which achieves 99.2% word accuracy without explicitly modeling fingers.

- **Real-time american sign language recognition from video using hidden markov models**[ISCV 1995] [[paper]](https://ieeexplore.ieee.org/document/477012)  
  **Authors:** Starner, Thad; Pentland, Alex  
  **Introduction:** This paper proposes a real-time sentence-level American Sign Language recognition system based on Hidden Markov Models, which achieves high-accuracy word recognition without explicitly modeling fingers.

- **Large vocabulary sign language recognition based on fuzzy decision trees**[Trans. S.M.C. 2004] [[paper]](https://ieeexplore.ieee.org/document/1288342)  
  **Authors:** Gaolin Fang, Wen Gao, Debin Zhao  
  **Introduction:** This paper proposes a fuzzy decision tree method based on heterogeneous classifiers, which realizes large-vocabulary sign language recognition through hierarchical screening and SOFM/HMM classifiers, significantly reducing recognition time and improving accuracy.

- **American sign language (ASL) recognition based on Hough transform and neural networks**[ESA 2007] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417405003040)  
  **Authors:** Munib Qutaishat, Habeeb Moussa, Takruri Bayan, Al-Malik Hiba Abed  
  **Introduction:** This paper proposes a bare-hand ASL static gesture recognition system based on Hough transform and neural network, which can recognize letters and symbols in a natural way without gloves or markers.

- **Sign transition modeling and a scalable solution to continuous sign language recognition for real-world applications**[TACCESS 2016] [[paper]](https://dl.acm.org/doi/10.1145/2850421)  
  **Authors:** Li, Kehuang, Zhou, Zhengyu, Lee, Chin-Hui  
  **Introduction:** A continuous sign language recognition method based on hidden Markov modeling is proposed. By collecting data through low-cost gloves, robust modeling of gestures and their conversions is achieved. With a vocabulary of 510 words, the word accuracy rate reaches 87.4% and supports real-time recognition.

### Data-driven spatiotemporal deep learning
- **Persian sign language (PSL) recognition using wavelet transform and neural networks**[ESA 2011] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417410008523)  
  **Authors:** Karami, Ali, Zanj, Bahman, Sarkaleh, Azadeh Kiani  
  **Introduction:** This paper proposes a Persian sign language static letter recognition system based on wavelet transform and multi-layer perceptron neural network, which achieves an average classification accuracy of 94.06% on bare hand images.

- **Sign language recognition using convolutional neural networks**[ECCV 2015] [[paper]](https://link.springer.com/chapter/10.1007/978-3-319-16178-5_40)  
  **Authors:** Pigou, Lionel, Dieleman, Sander, Kindermans, Pieter-Jan, Schrauwen, Benjamin  
  **Introduction:** This paper proposes an automatic sign language recognition system based on Kinect, CNN, and GPU acceleration, which achieves high-accuracy recognition of 20 Italian gestures and generalizes to unseen users and environments.

- **Attention-based 3D-CNNs for large-vocabulary sign language recognition**[TCSVT 2018] [[paper]](https://ieeexplore.ieee.org/document/8466903)  
  **Authors:** Huang, Jie, Zhou, Wengang, Li, Houqiang, Li, Weiping  
  **Introduction:** This paper proposes an attention-based 3D-CNN framework to automatically extract spatiotemporal features from videos and select key actions for efficient sign language recognition. 

- **A Deep Neural Framework for Continuous Sign Language Recognition by Iterative Training**[TMM 2019] [[paper]](https://ieeexplore.ieee.org/document/8598757)  
  **Authors:** Cui, Runpeng, Liu, Hu, Zhang, Changshui  
  **Introduction:** This paper proposes an iteratively optimized continuous sign language recognition framework based on deep convolutional neural networks and bidirectional recurrent neural networks, which significantly improves the recognition performance through multimodal fusion.

- **Hand Gesture Recognition for Sign Language Using 3DCNN**[Access 2020] [[paper]](https://ieeexplore.ieee.org/document/9078786)  
  **Authors:** Al-Hammadi, Muneer, Muhammad, Ghulam, Abdul, Wadood, Alsulaiman, Mansour, Bencherif, Mohamed A., Mekhtiche, Mohamed Amine  
  **Introduction:** This paper proposes a deep convolutional neural network gesture recognition method based on transfer learning, which achieves high-precision recognition on different sign language datasets. 

- **Vision-based hand gesture recognition using deep learning for the interpretation of sign language**[ESA 2021] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417421010484)  
  **Authors:** Sharma, Sakshi, Singh, Sukhwinder  
  **Introduction:** This paper proposes a compact and efficient convolutional neural network model that achieves nearly 100% gesture recognition accuracy on Indian Sign Language and American Sign Language datasets while being robust to rotation and scaling.  

- **ELM based two-handed dynamic turkish sign language (TSL) word recognition**[ESA 2021] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417421006461)  
  **Authors:** Katilmis, Zekeriya, Karakuzu, Cihan  
  **Introduction:** This paper proposes a Turkish Sign Language dynamic vocabulary recognition system based on Leap Motion and feature dimensionality reduction, using the ML-KELM classifier to achieve high accuracy and stable performance.  

- **An optimized generative adversarial network based continuous sign language classification**[ESA 2021] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417421007077)  
  **Authors:** Elakkiya, R, Vijayakumar, Pandi, Kumar, Neeraj  
  **Introduction:** This paper proposes a method based on hyperparameter optimized generative adversarial networks (H-GANs) for efficiently and accurately classifying manual and non-manual gestures of sign language in continuous videos, and improves recognition performance while maintaining low complexity.

- **Sign language recognition based on R (2+ 1) D with spatial-temporal-channel attention**[THMS 2022] [[paper]](https://ieeexplore.ieee.org/document/9702523)  
  **Authors:** Han, Xiangzu, Lu, Fei, Yin, Jianqin, Tian, Guohui, Liu, Jun  
  **Introduction:** An improved R(2+1)D model is proposed, combined with a lightweight spatiotemporal channel attention module, which effectively separates and enhances the spatial, temporal and channel features in sign language videos, thereby improving sign language recognition performance on multiple datasets.

- **Hear Sign Language: A Real-Time End-to-End Sign Language Recognition System**[TMC 2022] [[paper]](https://ieeexplore.ieee.org/document/9261114)  
  **Authors:** Wang, Zhibo, Zhao, Tengda, Ma, Jinxin, Chen, Hongkai, Liu, Kaixin, Shao, Huajie, Wang, Qian, Ren, Ju  
  **Introduction:** This paper proposes DeepSLR, an end-to-end real-time sign language recognition system based on an attention mechanism. It uses multi-channel IMU and sEMG sensors to capture arm and finger movements, achieving high-accuracy continuous sign language translation without gesture segmentation.

- **Spatial--temporal enhanced network for continuous sign language recognition**[TCSVT 2023] [[paper]](https://ieeexplore.ieee.org/document/10185608)  
  **Authors:** Yin, Wenjie, Hou, Yonghong, Guo, Zihui, Liu, Kailin  
  **Introduction:** This paper proposes a spatiotemporal enhancement network that effectively extracts gesture and facial dynamic features through spatial-visual alignment and temporal feature difference modules, thereby improving the performance of continuous sign language recognition.

- **A hybrid approach for Bangla sign language recognition using deep transfer learning model with random forest classifier**[ESA 2023] [[paper]](https://www.sciencedirect.com/science/article/pii/S0957417422019327)  
  **Authors:** Das, Sunanda, Imtiaz, Md Samir, Neom, Nieb Hasan, Siddique, Nazmul, Wang, Hui  
  **Introduction:** This paper proposes a hybrid model of convolutional neural networks and random forest classifiers that combines deep transfer learning to achieve efficient automatic recognition of Bengali sign language (numbers and letters), and verifies its accuracy and feasibility on a public dataset.

- **Continuous sign language recognition for hearing-impaired consumer communication via self-guidance network**[TCE 2023] [[paper]](https://ieeexplore.ieee.org/document/10359133)  
  **Authors:** Xue, Wanli, Kang, Ze, Guo, Leming, Yang, Shourui, Yuan, Tiantian, Chen, Shengyong  
  **Introduction:** This paper proposes a self-guided network (SGN) framework to enhance the feature representation of continuous sign language recognition through spatial, temporal and category constraints, significantly improving the recognition performance on multiple CSLR benchmark datasets.

- **Interactive attention and improved GCN for continuous sign language recognition**[BSPC 2023] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S1746809423003646)  
  **Authors:** Guo, Qi, Zhang, Shujun, Tan, Liwei, Fang, Ke, Du, Yinghao  
  **Introduction:** This paper proposes a continuous sign language recognition method based on an interactive attention mechanism and an improved hand movement decoupling GCN. By fusing RGB and skeleton features and introducing a cascaded attention module, efficient spatiotemporal modeling of bimanual movements is achieved, thereby improving CSLR performance.

- **A two-stream sign language recognition network based on keyframe extraction method**[ESA 2024] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417424011345)  
  **Authors:** Liu, Tianyu, Tao, Tangfei, Zhao, Yizhe, Zhu, Jieli  
  **Introduction:** This paper proposes a sign language recognition method that combines keyframe extraction and a full sign language dual-stream network, which effectively summarizes video information and focuses on hand features, achieving excellent performance on multiple sign language datasets.

- **Spatial Temporal Aggregation for Efficient Continuous Sign Language Recognition**[TETCI 2024] [[paper]](https://ieeexplore.ieee.org/document/10488467)  
  **Authors:** Hu, Lianyu, Gao, Liqing, Liu, Zekang, Liu, Wei  
  **Introduction:** This paper proposes the STAgg method, which effectively reduces the computational and memory overhead of continuous sign language recognition by aggregating similar frames into a unified representation, while maintaining or even improving the recognition accuracy.

- **American Sign language fingerspelling recognition in the wild with spatio temporal feature extraction and multi-task learning**[ESA 2024] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417423034036)  
  **Authors:** Pannattee, Peerawat, Kumwilaisak, Wuttipong, Hansakunbuntheung, Chatchawarn, Thatphithakkul, Nattanun, Kuo, C-C Jay  
  **Introduction:** In this paper, we propose a fingerspelling recognition system that combines VTCNN temporal feature extraction, multi-task learning, supervised contrastive learning, and CTC/attention joint decoding, achieving state-of-the-art performance on the ChicagoFSWild and ChicagoFSWild+ datasets.

- **An ultra-low-computation model for understanding sign languages**[ESA 2024] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417424006481)  
  **Authors:** Fallah, Mohammad K, Najafi, Mohammadreza, Gorgin, Saeid, Lee, Jeong-A  
  **Introduction:** A low-computation sign language recognition method is proposed. Through image abstraction and fully connected neural networks, high-precision recognition (ASL, ISL, BSL) is achieved while significantly reducing model size and computational overhead.

- **Word separation in continuous sign language using isolated signs and post-processing**[ESA 2024] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S095741742400561X)  
  **Authors:** Rastgoo, Razieh, Kiani, Kourosh, Escalera, Sergio  
  **Introduction:** A two-stage model is proposed, which uses CNN+SVD+LSTM trained with isolated gestures and a post-processing algorithm to achieve efficient detection and recognition of isolated gesture boundaries in continuous sign language videos.

- **StepNet: Spatial-temporal part-aware network for isolated sign language recognition**[TOMM 2024] [[paper]](https://arxiv.org/abs/2212.12857)  
  **Authors:** Shen, Xiaolong, Zheng, Zhedong, Yang, Yi  
  **Introduction:** We propose StepNet, an RGB-based spatiotemporal component-aware network that captures hand and facial features through component-level spatial and temporal modeling, achieving competitive accuracy on multiple sign language recognition benchmarks.

- **MSE-GCN: A Multiscale Spatiotemporal Feature Aggregation Enhanced Efficient Graph Convolutional Network for Dynamic Sign Language Recognition**[TETCI 2024] [[paper]](https://ieeexplore.ieee.org/document/10799160)  
  **Authors:** Naz, Neelma, Sajid, Hasan, Ali, Sara, Hasan, Osman, Ehsan, Muhammad Khurram  
  **Introduction:** This paper proposes a multi-scale efficient graph convolutional network (MSE-GCN), combined with a spatiotemporal joint attention mechanism, to achieve high-precision and low-computational cost feature extraction and classification in skeleton sign language recognition.

### Multimodal collaborative semantic enhancement
- **Large-Vocabulary Continuous Sign Language Recognition Based on Transition-Movement Models**[ITSMC 2007] [[paper]](https://ieeexplore.ieee.org/document/4032919)  
  **Authors:** Fang, Gaolin; Gao, Wen; Zhao, Debin  
  **Introduction:** This paper proposes a transition movement model (TMM) method to effectively handle adjacent gesture transitions in large-vocabulary continuous sign languages ​​through dynamic clustering and iterative segmentation.

- **Taiwan sign language (TSL) recognition based on 3D data and neural networks**[ESA 2009] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417407005210)  
  **Authors:** Lee, Yung-Hui, Tsai, Cheng-Yueh  
  **Introduction:** The system based on 3D gesture data and neural network proposed in this paper can accurately recognize Taiwanese Sign Language static gestures with an average accuracy of 96.58% and has good robustness.

- **Discriminative Exemplar Coding for Sign Language Recognition With Kinect**[T-Cybern 2013] [[paper]](https://ieeexplore.ieee.org/document/6544211)  
  **Authors:** Sun, Chao; Zhang, Tianzhu; Bao, Bing-Kun; Xu, Changsheng; Mei, Tao  
  **Introduction:** This paper proposes a Discriminative Sample Coding (DEC) method to select the most discriminative samples from Kinect videos through multi-instance learning, thereby achieving efficient recognition of sign language videos.

- **Semantic boundary detection with reinforcement learning for continuous sign language recognition**[TCSVT 2020] [[paper]](https://ieeexplore.ieee.org/document/9106402)  
  **Authors:** Wei, Chengcheng; Zhao, Jian; Zhou, Wengang; Li, Houqiang  
  **Introduction:** This paper proposes a semantic boundary detection method based on reinforcement learning, which accurately aligns video frames and sign language words through multi-scale feature learning to achieve weakly supervised continuous sign language recognition.

- **L-sign: Large-vocabulary sign gestures recognition system**[THMS 2022] [[paper]](https://ieeexplore.ieee.org/document/9714154)  
  **Authors:** Zheng, Zhiwen; Wang, Qingshan; Yang, Dejun; Wang, Qi; Huang, Wei; Xu, Yinlong  
  **Introduction:** This paper proposes an L-sign system based on a smart bracelet. It uses entropy matching to segment gestures and combines a three-branch CNN with semantic voting to achieve large-vocabulary sign language recognition. The average accuracy of 200 commonly used gestures exceeds 90%.

- **A sign language recognition system with pepper, lightweight-transformer, and LLM**[arXiv 2023] [[paper]](https://arxiv.org/abs/2309.16898)  
  **Authors:** Lim, JongYoon; Sa, Inkyu; MacDonald, Bruce; Ahn, Ho Seok  
  **Introduction:** This study proposes a method that combines lightweight deep learning with a large language model, enabling the Pepper robot to understand American Sign Language and generate natural gesture and voice responses, thereby achieving efficient and intuitive non-verbal human-computer interaction.

- **The ASL Dataset for Real-Time Recognition and Integration with LLM Services**[IJET 2024] [[paper]](https://ijet.ise.pw.edu.pl/index.php/ijet/article/view/10.24425-ijet.2024.152513)  
  **Authors:** Chwesiuk, Michał, Popis, Piotr  
  **Introduction:** This study verified the high accuracy of American Sign Language (ASL) recognition using a multi-user gesture image dataset and machine learning methods, demonstrating that individual differences have limited impact on performance, providing new insights for improving the reliability of automatic sign language recognition systems and barrier-free communication technologies.

- **Cross-Modal Adaptive Prototype Learning for Continuous Sign Language Recognition**[T-CSVT 2025] [[paper]](https://ieeexplore.ieee.org/document/10896751)  
  **Authors:** Wei, Dong; Yang, Xu-Hua; Weng, Yiyang; Lin, Xuanyu; Hu, Hongxiang; Liu, Sheng  
  **Introduction:** This paper proposes CAP-SLR, a continuous sign language recognition method that combines keyframe extraction, multi-scale convolutional attention, and cross-modal adaptive prototype learning. It effectively alleviates the problems of spatiotemporal redundancy and weak supervision, and achieves high-precision recognition on multiple datasets.

- **A structure-based disentangled network with contrastive regularization for sign language recognition**[ESA 2025] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417425002453)  
  **Authors:** Gao, Liqing; Zhu, Lei; Hu, Lianyu; Shi, Peng; Wan, Liang; Feng, We  
  **Introduction:** This paper proposes a structured decoupled network (SD-Net), which improves the accuracy of sign language recognition by separating sentence templates and content entities and fusing their features while utilizing temporal context modeling.
  
- **SCOPE: Sign Language Contextual Processing with Embedding from LLMs**[AAAI 2025] [[paper]](https://arxiv.org/abs/2409.01073)  
  **Authors:** Liu, Yuqi; Zhang, Wenqian; Ren, Sihan; Huang, Chengyu; Yu, Jingyi; Xu, Lan  
  **Introduction:** This paper proposes SCOPE, a visual sign language recognition and translation framework that combines conversational context and large-scale language models, and releases a new dataset containing 72 hours of Chinese sign language conversations.
      
## Sign language translation
### Rule-based statistical visual pattern mapping
- **Increasing adaptability of a speech into sign language translation system**[ESA 2013] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417412010202)  
  **Authors:** López-Ludeña, Verónica; San-Segundo, Rubén; Morcillo, Carlos González; López, Juan Carlos; Muñoz, José M Pardo  
  **Introduction:** This paper proposes an improved speech-to-sign language translation system that achieves rapid adaptation to new tasks or new semantic domains by enhancing the adaptability of speech recognition, machine translation, and 3D animation modules, while maintaining low error rate and high translation quality while reducing adaptation costs.

- **Statistical Machine Translation for Greek to Greek Sign Language Using Parallel Corpora Produced via Rule-Based Machine Translation**[CIMA@ICTAI 2018] [[paper]](https://www.semanticscholar.org/paper/Statistical-Machine-Translation-for-Greek-to-Greek-Kouremenos-Ntalianis/bfe80a47199e1f5be29fe58eb7a7c9410dffbdd5)  
  **Authors:** Dimitris Kouremenos; Klimis S. Ntalianis; Georgios Siolas; Andreas Stafylopatis  
  **Introduction:** This paper presents a Rule-Based Machine Translation (RBMT) system that enables high-quality Greek text to Greek Sign Language (GSL) glossed corpus creation, facilitating statistical machine translation and overcoming challenges related to GSL's lack of written form and grammar knowledge.
  
- **Rule-based Machine Translation into Ukrainian Sign Language Using Concept Dictionary**[ICTERI 2019] [[paper]](https://www.semanticscholar.org/paper/Rule-based-Machine-Translation-into-Ukrainian-Sign-Lozynska-Davydov/1fddc314d4b8fd9e696005e2bea9e8181f384b6d)  
  **Authors:** Olga Lozynska, Maksym Davydov, Volodymyr V. Pasichnyk, and Nataliia Veretennikova  
  **Introduction:** This paper proposes a rule-based translation method for Ukrainian spoken to sign language based on a concept dictionary. By establishing a five-category relationship model between words, symbols, and concepts, it effectively addresses the challenges of grammatical differences and non-one-to-one translation between the two languages.

### Deep learning-based temporal-spatial pattern modeling
- **Hierarchical Recurrent Deep Fusion Using Adaptive Clip Summarization for Sign Language Translation**[TIP 2019] [[paper]](https://ieeexplore.ieee.org/document/8846585)  
  **Authors:** Guo, Dan; Zhou, Wengang; Li, Anyang; Li, Houqiang; Wang, Meng  
  **Introduction:** This paper proposes a hierarchical deep recurrent fusion (HRF) framework, which combines adaptive temporal coding with multimodal fusion to effectively improve the performance of vision-based sign language translation.

- **Multi-Information Spatial–Temporal LSTM Fusion Continuous Sign Language Neural Machine Translation**[Access 2020] [[paper]](https://ieeexplore.ieee.org/document/9265176)  
  **Authors:** Qinkun Xiao, Xin Chang, Xue Zhang, Xing Liu  
  **Introduction:** This paper proposes a bidirectional spatiotemporal LSTM framework with attention (Bi-ST-LSTM-A) to achieve high-precision continuous sign language recognition without the need for sentence segmentation and tedious annotation.

- **Improving Sign Language Translation With Monolingual Data by Sign Back-Translation**[CVPR 2021] [[paper]](https://arxiv.org/abs/2105.12397)  
  **Authors:** Zhou, Hao; Zhou, Wengang; Qi, Weizhen; Pu, Junfu; Li, Houqiang  
  **Introduction:** This paper introduces a back-translation method, SignBT, for synthesizing parallel sign language text data from large-scale spoken text, and contributes a new large-scale continuous SLT dataset CSL-Daily.

- **Graph-Based Multimodal Sequential Embedding for Sign Language Translation**[T-MM 2021] [[paper]](https://ieeexplore.ieee.org/document/9556136)  
  **Authors:** Tang, Shengeng; Guo, Dan; Hong, Richang; Wang, Meng  
  **Introduction:** This paper proposes MSeqGraph, a graph-based multimodal sequence embedding network for sign language translation (SLT) that captures intra- and inter-modal correlations, learns temporal cues through graph embedding units, and employs hierarchical stacking and temporal decoding for effective weakly supervised SLT.

- **Conditional Sentence Generation and Cross-Modal Reranking for Sign Language Translation**[T-MM 2021] [[paper]](https://ieeexplore.ieee.org/document/9447976)  
  **Authors:** Zhao, Jian; Qi, Weizhen; Zhou, Wengang; Duan, Nan; Zhou, Ming; Li, Houqiang  
  **Introduction:** This paper proposes a novel SLT framework that leverages a large-scale text corpus to generate accurate spoken translations from sign language videos through word verification, conditional sentence generation, and cross-modal reordering.

- **Spatial-Temporal Multi-Cue Network for Sign Language Recognition and Translation**[T-MM 2021] [[paper]](https://ieeexplore.ieee.org/document/9354538)  
  **Authors:** Zhou, Hao; Zhou, Wengang; Zhou, Yun; Li, Houqiang  
  **Introduction:** This paper proposes a Spatio-Temporal Multi-Cues (STMC) network that utilizes spatial and temporal multi-cue modules with joint optimization and segmented attention to enhance vision-based sign language understanding.

 - **MLSLT: Towards Multilingual Sign Language Translation**[CVPR 2022] [[paper]](https://ieeexplore.ieee.org/document/9878501) [[code]](https://github.com/MLSLT/SP-10)  
  **Authors:** Yin, Aoxiong; Zhao, Zhou; Jin, Weike; Zhang, Meng; Zeng, Xingshan; He, Xiaofei  
  **Introduction:** This study introduces ML-SLT, the first multilingual sign language translation model, leveraging dynamic routing mechanisms to efficiently share parameters across languages, achieving superior performance on the new Spreadthesign-Ten dataset and enabling zero-shot translation between sign and spoken languages.
 
- **Automatic translation of sign language with multi-stream 3D CNN and generation of artificial depth maps**[ESA 2023] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417422024125)  
  **Authors:** De Castro, Giulia Zanon; Guerra, Rubia Reis; Guimarães, Frederico Gadelha  
  **Introduction:** This study proposes a multi-stream deep learning model using 3D CNN and GAN-generated depth maps to accurately identify BSL, ISL, and KSL, achieving high performance and providing interpretable insights into key features.

- **Overcoming Modality Bias in Question-Driven Sign Language Video Translation**[T-CSVT 2024] [[paper]](https://ieeexplore.ieee.org/document/10572012)[[code]](https://github.com/glq-1992/QSL)  
  **Authors:** Gao, Liqing; Lyu, Fan; Shi, Peng; Zhu, Lei; Pu, Junfu; Wan, Liang; Feng, Wei  
  **Introduction:** The "Gloss-Bridged Translator" (GBT) method is proposed to bridge question text and sign language video through sign language annotations to achieve cross-modal semantic alignment. 

- **Improving End-to-end Sign Language Translation with Adaptive Video Representation Enhanced Transformer**[T-CSVT 2024] [[paper]](https://ieeexplore.ieee.org/document/10466749)[[code]](https://github.com/LzDddd/AVRET)  
  **Authors:** Liu, Zidong; Wu, Jiasong; Shen, Zeyu; Chen, Xin; Wu, Qianyu; Gui, Zhiguo; Senhadji, Lotfi; Shu, Huazhong  
  **Introduction:** This study proposes AVRET, an adaptive video representation enhanced Transformer, which achieves end-to-end continuous sign language translation through adaptive masks, local segment self-attention, and adaptive fusion modules. 

- **Sign Language to Text Translation with Computer Vision: Bridging the Communication Gap**[ICDXA 2024] [[paper]](https://ieeexplore.ieee.org/document/10470532)  
  **Authors:** Thong, So Xue; Tan, Eng Lip; Goh, Ching Pang  
  **Introduction:** This study proposes a real-time sign language translation system based on CNN and LSTM to achieve high-precision gesture recognition and sentence generation.

- **C2RL: Content and Context Representation Learning for Gloss-free Sign Language Translation and Retrieval**[T-CSVT 2025] [[paper]](https://arxiv.org/abs/2408.09949)  
  **Authors:** Chen, Zhigang; Zhou, Benjia; Huang, Yiqing; Wan, Jun; Hu, Yibo; Shi, Hailin; Liang, Yanyan; Lei, Zhen; Zhang, Du  
  **Introduction:** In this paper, we propose C2RL, an unannotated sign language representation learning method, which achieves robust feature extraction from sign language videos by jointly optimizing implicit content learning and explicit context learning.

### Large Language Model (LLM)-based sign language translation
- **Leveraging Large Language Models With Vocabulary Sharing For Sign Language Translation**[ICASSPW 2023] [[paper]](https://ieeexplore.ieee.org/document/10193533)  
  **Authors:** Lee, Huije; Kim, Jung-Ho; Hwang, Eui Jun; Kim, Jaewoo; Park, Jong C.  
  **Introduction:** This paper introduces Ko-GPT-Trinity-1.2B+VS, a vocabulary-sharing large-scale language model for Korean Sign Language translation, which effectively addresses the problem of scarce SLT data.

- **Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation**[arXiv 2024] [[paper]](https://arxiv.org/abs/2403.12556)  
  **Authors:** Zhigang Chen; Benjia Zhou; Jun Li; Jun Wan; Zhen Lei; Ning Jiang; Quan Lu; Guoqing Zhao  
  **Introduction:** This paper proposes the FLa-LLM method to achieve annotation-free sign language translation by training a visual encoder and a large language model in stages.

- **An Innovative System for Real-Time Translation from American Sign Language (ASL) to Spoken English using a Large Language Model (LLM)**[UEMCON 2024] [[paper]](https://www.semanticscholar.org/paper/An-Innovative-System-for-Real-Time-Translation-from-Johnson-Rashad/b9d4c9deffd1ffbbd091111c6d97200b95942ea6)  
  **Authors:** Johnson, Larry Emerson; Rashad, Sherif  
  **Introduction:** This study proposes a real-time American Sign Language (ASL) to English translation system that combines Leap Motion and a large language model to improve the communication efficiency of hearing-impaired people in daily life.
       
- **Enhanced Sign Language Translation between American Sign Language (ASL) and Indian Sign Language (ISL) Using LLMs**[arXiv 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/11113285)  
  **Authors:** Malay Kumar, S. Sarvajit Visagan, Tanish Sarang Mahajan, Anisha Natarajan  
  **Introduction:** This study proposes a hybrid deep learning framework that enables real-time translation between ASL and ISL, achieving high gesture recognition and text correction accuracy while generating smooth sign language videos for seamless communication.

- **Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation**[arXiv 2024] [[paper]](https://arxiv.org/abs/2405.04164)[[code]](https://github.com/ryanwongsa/Sign2GPT)  
  **Authors:** Ryan Wong; Necati Cihan Camgoz; Richard Bowden  
  **Introduction:** This paper proposes the Sign2GPT framework, which enables annotation-free sign language translation and significantly improves performance on public datasets by leveraging pre-trained vision-language models and lightweight adapters.

- **Using an LLM to Turn Sign Spottings into Spoken Language Sentences**[arXiv 2024] [[paper]](https://arxiv.org/html/2403.10434v1)  
  **Authors:** Ozge Mercanoglu Sincan; Necati Cihan Camgoz; Richard Bowden  
  **Introduction:** This paper proposes Spotter+GPT, a hybrid approach that combines a sign language recognizer with a pre-trained large language model for efficient translation of sign language videos into coherent spoken sentences.

- **LLMs are Good Sign Language Translators**[CVPR 2024] [[paper]](https://arxiv.org/abs/2404.00925)  
  **Authors:** Gong, Jia; Foo, Lin Geng; He, Yixuan; Rahmani, Hossein; Liu, Jun  
  **Introduction:** This paper proposes SignLLM, a framework that converts sign language videos into language-like representations using vector-quantized visual tokens and codebook alignment, enabling large language models to perform accurate sign language translation without annotations.

- **Enhancing Sign Language Interpretation with Multiheaded CNN, Hand Landmarks and Large Language Model(LLM)**[FMLDS 2024] [[paper]](https://ieeexplore.ieee.org/document/10874050)  
  **Authors:** Chaitanya Kakade; Nidhi Kadam; Vishal Kaira; Rishi Kewalya  
  **Introduction:** This study proposed a sign language recognition and translation system that combines a multi-head convolutional neural network and a large language model, which achieves accurate recognition of gestures in different contexts and converts them into grammatically correct and easy-to-understand text.

- **LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing Online Learners**[arXiv 2024] [[paper]](https://arxiv.org/abs/2411.09873)  
  **Authors:** Cheng, Haocong; Chen, Si; Perdriau, Christopher; Huang, Yun  
  **Introduction:** This paper investigates how an intelligent tutoring system based on a large language model can improve the interactive experience of deaf-hard of hearing learners through an AI tutor with experience in DHH education, and offers recommendations for designing more inclusive LLM systems.

- **Enhancing Human-Robot Interaction: Integrating ASL Recognition and LLM-Driven Co-Speech Gestures in Pepper Robot with a Compact Neural Network**[UR 2024] [[paper]](https://ieeexplore.ieee.org/document/10597463)  
  **Authors:** Lim, JongYoon, Sa, Inkyu, MacDonald, Bruce A, Ahn, Ho Seok  
  **Introduction:** This study proposes a system that combines a compact deep neural network with a large language model, enabling the humanoid robot Pepper to efficiently understand and generate American Sign Language, achieving smooth human-computer non-verbal interaction.

- **Beyond Words: AuralLLM and SignMST-C for Sign Language Production and Bidirectional Accessibility**[arXiv 2025] [[paper]](https://arxiv.org/abs/2501.00765)  
  **Authors:** Yulong Li; Yuxuan Zhang; Feilong Tang; Ming Hu; Zhixiang Lu; Haochen Xue; Jianghao Wu; Mian Zhou; Kang Dang; Chong Li; Yifang Wang; Imran Razzak; Jionglong Su  
  **Introduction:** This study establishes a new benchmark for SLP and SLT tasks by introducing large sign language datasets (CNText2Sign and CNSign) and high-performance models (AuraLLM and SignMST-C).

- **GFTLS-SLT: Gloss-Free Transformer Based Lexical and Semantic Awareness Framework for Multimodal Sign Language Translation**[T-MM 2025] [[paper]](https://ieeexplore.ieee.org/document/10891585)  
  **Authors:** Zhang, Jiangtao; Wang, Qingshan; Wang, Qi  
  **Introduction:** This paper proposes GFTLS-SLT, an annotation-free multimodal sign language translation framework that achieves gesture boundary understanding and semantic alignment through vocabulary and global semantic awareness modules, achieving the performance of annotated supervised methods on public datasets.

- **HYBRID MODEL COLLABORATION FOR SIGN LANGUAGE TRANSLATION WITH VQ-VAE AND RAG ENHANCED LLMS**[ICLR 2025] [[paper]](https://openreview.net/forum?id=7kRFnSFN89)[[code]](https://github.com/VRG-SLT/VRG-SLT-demos)  
  **Authors:** Ma, Jian; Wang, Wenguan; Yang, Yi; Guan, Weili; Zheng, Feng  
  **Introduction:** This paper proposes a VRG-SLT framework that encodes sign language sequences into discrete gesture representations through a hierarchical VQ-VAE and combines fine-tuning a large language model with retrieval-augmented generation to achieve high-quality sign language to spoken language translation.
  
## Sign language production
### Generative Model-Driven Sign Language Generation
- **Spoken Spanish generation from sign language**[I.C. 2010] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S095354380900099X)  
  **Authors:** San-Segundo, Rubén; Pardo, José Manuel; Ferreiros, Javier; Sama, Valentín; Barra-Chicote, Roberto; Lucas, Juan Manuel; Sánchez, D; García, Antonio  
  **Introduction:** This paper proposes a system for converting Spanish Sign Language (LSE) writing into Spanish speech and constructs the first Spanish-LSE parallel corpus.

- **Developing a statistical Turkish sign language translation system for primary school students**[INISTA 2019] [[paper]](https://ieeexplore.ieee.org/document/8778246)  
  **Authors:** Buz, Buse; Güngör, Tunga  
  **Introduction:** This paper proposes the first statistical machine translation method from Turkish to Turkish Sign Language for elementary school students and verifies its effectiveness using a parallel corpus.

- **Text2Sign: towards sign language production using neural machine translation and generative adversarial networks**[IJCVI 2020] [[paper]](https://link.springer.com/article/10.1007/s11263-019-01281-2)  
  **Authors:** Stoll, Stephanie; Camgoz, Necati Cihan; Hadfield, Simon; Bowden, Richard  
  **Introduction:** This paper proposes a new method that combines neural machine translation and generative adversarial networks to directly convert text into realistic sign language videos, without the need for traditional virtual humans and a large amount of labeled data, achieving end-to-end sign language generation. 

- **Neural sign language synthesis: Words are our glosses**[WACV 2020] [[paper]](https://ieeexplore.ieee.org/document/9093516)  
  **Authors:** Zelinka, Jan; Kanis, Jakub  
  **Introduction:** In this paper, we propose a fully end-to-end text-to-sign language skeleton generation method that only uses freely available broadcast data and does not require manual annotation or video segmentation. We achieve effective sign language synthesis through a soft non-monotonic attention mechanism and feature comparison.

- **Skeleton-based Chinese sign language recognition and generation for bidirectional communication between deaf and hearing people**[NN 2020] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S089360802030040X)  
  **Authors:** Xiao Qinkun, Qin Minying, Yin Yuting  
  **Introduction:** This paper proposes a bidirectional Chinese sign language recognition and generation framework based on skeleton and recurrent neural networks, which improves the skeleton sequence diversity and recognition accuracy through a two-level probabilistic generation model.

- **Everybody sign now: Translating spoken language to photo realistic sign language video**[arXiv 2020] [[paper]](https://arxiv.org/abs/2011.09846)  
  **Authors:** Saunders Ben, Camgoz Necati Cihan, Bowden Richard  
  **Introduction:** This paper proposes SignGAN, a system for generating realistic continuous sign language videos directly from spoken language. It achieves high-quality and controllable sign language video generation by converting skeleton poses through Transformer+MDN and combining pose-conditioned human synthesis and keypoint loss.

- **Adversarial training for multi-channel sign language production**[arXiv 2020] [[paper]](https://arxiv.org/abs/2008.12405)  
  **Authors:** Saunders Ben, Camgoz Necati Cihan, Bowden Richard  
  **Introduction:** This paper proposes an adversarial multi-channel sign language generation method, which achieves realistic and complete sign language generation through the game between the Transformer generator and the conditional discriminator, combining manual and non-manual features.

- **Signsynth: Data-driven sign language video generation**[ECCV 2020] [[paper]](https://www.researchgate.net/publication/343851233_SignSynth_Data-Driven_Sign_Language_Video_Generation)  
  **Authors:** Stephanie Stoll, Simon Hadfield, Richard Bowden  
  **Introduction:** This paper proposes SignSynth, an end-to-end sign language video generation method that achieves high-definition and natural sign language video generation through gloss2pose and pose2video networks.

- **Generation of indian sign language by sentence processing and generative adversarial networks**[ICISS 2020] [[paper]](https://ieeexplore.ieee.org/document/9315979)  
  **Authors:** Neel Vasani, Pratik Autee, Samip Kalyani, Ruhina Karani  
  **Introduction:** This paper proposes a method based on sentence preprocessing and generative adversarial networks to convert input sentences into Indian sign language annotations and generate corresponding skeleton video frames, thereby realizing sentence-to-sign language video conversion.

- **Progressive transformers for end-to-end sign language production**[ECCV 2020] [[paper]](https://arxiv.org/abs/2004.14874) [[code]](https://github.com/BenSaunders27/ProgressiveTransformersSLP)  
  **Authors:** Ben Saunders, Necati Cihan Camgoz, Richard Bowden  
  **Introduction:** This paper proposes a Progressive Transformers architecture to convert text into a continuous 3D sign language skeleton sequence through an end-to-end or stacked network.

- **Towards fast and high-quality sign language production**[ACM MM 2021] [[paper]](https://dl.acm.org/doi/10.1145/3474085.3475463)  
  **Authors:** Wencan Huang, Wenwen Pan, Zhou Zhao, Qi Tian  
  **Introduction:** This paper proposes a non-autoregressive sign language gesture generation model that combines an external comparator and a spatiotemporal graph convolutional generator to achieve parallel and high-quality sign language gesture generation.

- **Signing at scale: Learning to co-articulate signs for large-scale photo-realistic sign language production**[CVPR 2022] [[paper]](https://arxiv.org/abs/2203.15354) [[code]](https://github.com/BenSaunders27/meineDGS-Translation-Protocols?utm_source=catalyzex.com)  
  **Authors:** Saunders Ben, Camgoz Necati Cihan, Bowden Richard  
  **Introduction:** This paper proposes a large-scale sign language generation method that combines FS-Net and SignGAN. By learning dictionary sign language co-articulation and generating realistic skeleton videos, it achieves fluent and understandable sign language sequences in a wide range of language domains.

- **Dynamic GAN for high-quality sign language video generation from skeletal poses using generative adversarial networks**[SC 2022] [[paper]](https://link.springer.com/article/10.1007/s00500-022-07014-x)  
  **Authors:** B Natarajan, R Elakkiya  
  **Introduction:** This paper proposes Dynamic GAN, a generative framework based on skeletal poses and character images, to generate high-quality sign language videos without animation.

- **Development of an end-to-end deep learning framework for sign language recognition, translation, and video generation**[Access 2022] [[paper]](https://ieeexplore.ieee.org/document/9905589)  
  **Authors:** B Natarajan, E Rajalakshmi, R Elakkiya, Ketan Kotecha, Ajith Abraham  
  **Introduction:** This paper proposes a complete framework that combines CNN+Bi-LSTM gesture recognition with NMT+dynamic GAN sign language generation to achieve high-accuracy text recognition and high-quality sign language video generation.

- **Signnet ii: A transformer-based two-way sign language translation model**[TPAMI 2022] [[paper]](https://ieeexplore.ieee.org/document/9999492)  
  **Authors:** Lipisha Chaudhary, Tejaswini Ananthanarayana, Enjamamul Hoq, Ifeoma Nwogu  
  **Introduction:** This paper introduces SignNet II, a bidirectional Transformer-based architecture for sign language processing that improves bidirectional translation performance by jointly training sign-to-text and text-to-sign networks and keypoint embedding metrics.

- **A Pyramid Semi-Autoregressive Transformer with Rich Semantics for Sign Language Production**[Sensors 2022] [[paper]](https://www.mdpi.com/1424-8220/22/24/9606)  
  **Authors:** Zhenchao Cui, Ziang Chen, Zhaoxin Li, Zhaoqi Wang  
  **Introduction:** This paper proposes the PSAT-RS model, which effectively generates natural sign language videos through a pyramid semi-autoregressive mechanism and rich semantic embeddings. It outperforms existing autoregressive and non-autoregressive methods in terms of speed and accuracy.

- **There and back again: 3d sign language generation from text using back-translation**[3DV 2022] [[paper]](https://ieeexplore.ieee.org/document/10044459)  
  **Authors:** Stephanie Stoll, Armin Mustafa, Jean-Yves Guillemaut  
  **Introduction:** This paper proposes the Text2Pose and Text2Mesh methods to automatically generate 2D pose and 3D sign language mesh sequences from text, and establishes the first benchmark for this task.

- **Spatial-Temporal Consistency Constraints for Chinese Sign Language Synthesis**[ICCADCG 2023] [[paper]](https://link.springer.com/chapter/10.1007/978-981-99-9666-7_11)  
  **Authors:** Liqing Gao, Peidong Liu, Liang Wan, Wei Feng  
  **Introduction:** This paper proposes the STCC method, which achieves spatiotemporal consistency of sign language in video splicing by generating intermediate transition frames and hierarchical attention GAN, thereby improving the fluency, realism and comprehensibility of the synthesized video.

- **Sentence2SignGesture: a hybrid neural machine translation network for sign language video generation**[JAIHC 2023] [[paper]](https://link.springer.com/article/10.1007/s12652-021-03640-9)  
  **Authors:** B Natarajan, R Elakkiya, Moturi Leela Prasad  
  **Introduction:** This paper proposes an NMT model based on deep stacked GRU to achieve translation of spoken sentences into sign language annotations and generate sign language gesture videos through mapping.

- **Sign Language Motion Generation from Sign Characteristics**[Sensors 2023] [[paper]](https://www.mdpi.com/1424-8220/23/23/9365)  
  **Authors:** Manuel Gil-Martín, María Villa-Monedero, Andrzej Pomirski, Daniel Sáez-Trigueros, Rubén San-Segundo  
  **Introduction:** This paper proposes a Transformer-based sign language motion generation method. It generates highly detailed gestures using sign phonemes represented by HamNoSys, and combines it with a stop detection module to achieve high-precision motion generation and termination prediction.

- **Autoregressive sign language production: A gloss-free approach with discrete representations**[{arXiv 2023] [[paper]](https://arxiv.org/abs/2309.12179)  
  **Authors:** Eui Jun Hwang, Huije Lee, Jong C Park  
  **Introduction:** This paper proposes SignVQNet, a vector quantization-based unannotated sign language generation method that achieves highly coherent sign language generation through discrete representation and latent layer alignment.

- **Sindiff: Spoken-to-Sign Language Generation Based Transformer Diffusion Model**[PRL 2023] [[paper]](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4611530)  
  **Authors:** Wuyan Liang, Xiaolong Xu  
  **Introduction:** This paper proposes SinDiff, a Transformer-based diffuse sign language generation framework, which enables long-sequence spoken language-driven sign language generation through linearly biased multi-head attention and cross-modal alignment.

- **Sign stitching: a novel approach to sign language production**[arXiv 2024] [[paper]](https://arxiv.org/abs/2405.07663)  
  **Authors:** Harry Walsh, Ben Saunders, Richard Bowden  
  **Introduction:** This paper proposes a seven-step dictionary sign language concatenation method, combined with SignGAN to achieve natural and coherent text-to-photorealistic sign language video generation.

- **Turkish Sign Language Video Generation from Pose Sequences Using a Conditional GAN Model**[ICCSE 2024] [[paper]](https://ieeexplore.ieee.org/document/10773583)  
  **Authors:** Feyza Ozkan, Hatice Kubra Tekin, Hacer Yalim Keles  
  **Introduction:** This paper proposes a cGAN model based on pose sequences and reference images to achieve realistic generation of Turkish sign language videos while preserving finger and arm details.

- **Sign Language Video Generation from Text Using Generative Adversarial Networks**[OMNN 2024] [[paper]](https://link.springer.com/article/10.3103/S1060992X24700851)  
  **Authors:** R Sreemathy, Param Chordiya, Soumya Khurana, Mousami Turuk  
  **Introduction:** This paper proposes a cGAN-based Indian Sign Language video generation method to achieve video conversion from text to sign language and generate high-quality and controllable videos.

- **Sign Language Production Using Generative AI**[ICCIRT 2024] [[paper]](https://ieeexplore.ieee.org/document/10921902)  
  **Authors:** D Karthika Renuka, L Ashok Kumar, KR Harini, L Nithin, PR Rishi Khanna, J Suruthika  
  **Introduction:** This paper proposes a sign language generation method based on pix2pix CGAN, which converts skeletal key points into realistic movements and realizes natural gesture generation.

- **A Novel Approach for Sign Language Video Generation Using Deep Networks**[ICDSNS 2024] [[paper]](https://ieeexplore.ieee.org/document/10691162)  
  **Authors:** Sachin Kumar, B Deepa, T Kavitha, M Tamilselvi, V Sathiyapriya, B Natarajan  
  **Introduction:** This paper proposes VideoGAN, an enhanced GAN method for generating sign language gesture videos from text, achieving high-quality video generation and demonstrating excellent performance on diverse datasets from multiple countries.

- **A Gloss-Free Sign Language Production with Discrete Representation**[FG 2024] [[paper]](https://ieeexplore.ieee.org/document/10581980)  
  **Authors:** Eui Jun Hwang, Huije Lee, Jong C Park  
  **Introduction:** This paper proposes SignVQNet, which achieves annotation-free end-to-end sign language generation by combining discrete spatiotemporal representation of hand gestures with directed search and language models.

- **T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text**[arXiv 2024] [[paper]](https://arxiv.org/abs/2406.07119)  
  **Authors:** Aoxiong Yin, Haoyuan Li, Kai Shen, Siliang Tang, Yueting Zhuang  
  **Introduction:** This paper proposes the DVA-VAE model to implement dynamic vector quantization encoding of sign language sequences, combines it with a GPT-style generator for autoregressive sign language generation, and releases the large-scale PHOENIX-News dataset to improve performance.

- **Diversity-Aware Sign Language Production through a Pose Encoding Variational Autoencoder**[FG 2024] [[paper]](https://ieeexplore.ieee.org/document/10581951)  
  **Authors:** Mohamed Ilyes Lakhal, Richard Bowden  
  **Introduction:** This paper proposes a diversity-aware sign language generation method, which achieves high-quality sign language image generation with controlled appearance style through variational inference of pose and attribute conditions and a UNet architecture.

- **SeqαGAN: Sign Language Sequence Generation Based on Variational and Adversarial Learning**[TII 2024] [[paper]](https://www.researchgate.net/publication/379841883_SeqaGAN_Sign_Language_Sequence_Generation_Based_on_Variational_and_Adversarial_Learning)  
  **Authors:** Qinkun Xiao, Lu Li, Yilin Zhu  
  **Introduction:** This paper proposes SeqαGAN, a skeleton sign language sequence generation model that combines conditional VAE and conditional GAN ​​to improve sign language recognition performance by generating diverse and recognizable data.

- **Semantic-driven diffusion for sign language production with gloss-pose latent spaces alignment**[CVIU 2024] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S1077314224001310)  
  **Authors:** Sheng Chen, Qingshan Wang, Qi Wang  
  **Introduction:** This paper proposes the SDD-GPLA model to generate high-resolution sign language gestures through sign language vocabulary-gesture latent space alignment and semantically driven diffusion.

- **SignGen: End-to-End Sign Language Video Generation with Latent Diffusion**[ECCV 2024] [[paper]](https://eccv.ecva.net/virtual/2024/poster/2581)  
  **Authors:** Fan Qi, Yu Duan, Huaiwen Zhang, Changsheng Xu  
  **Introduction:** This paper proposes an end-to-end text-to-sign language video generation method, which achieves natural sign language generation with full body and facial expressions through multimodal conditions and motion perception framework.

- **Jointly harnessing prior structures and temporal consistency for sign language video generation**[TOMM 2024] [[paper]](https://dl.acm.org/doi/10.1145/3648368)  
  **Authors:** Yucheng Suo, Zhedong Zheng, Xiaohan Wang, Bang Zhang, Yi Yang  
  **Introduction:** This paper proposes STCNet to generate continuous and natural sign language videos through structure-aware skeleton and temporal consistency loss.

- **Pose-Guided Fine-Grained Sign Language Video Generation**[ECCV 2024] [[paper]](https://arxiv.org/abs/2409.16709)  
  **Authors:** Tongkai Shi, Lianyu Hu, Fanhua Shang, Jichao Feng, Peidong Liu, Wei Feng  
  **Introduction:** This paper proposes PGMM to generate fine-grained and temporally consistent sign language videos through a coarse-grained motion module and a posture fusion module.

- **Neural sign actors: a diffusion model for 3d sign language production from text**[CVPR 2024] [[paper]](https://arxiv.org/abs/2312.02702) [[code]](https://baltatzisv.github.io/neural-sign-actors/?utm_source=catalyzex.com)  
  **Authors:** Vasileios Baltatzis, Rolandos Alexandros Potamias, Evangelos Ververas, Guanxiong Sun, Jiankang Deng, Stefanos Zafeiriou  
  **Introduction:** This paper proposes a diffusion-based 4D sign language generation method, which uses an anatomically driven graph neural network to generate realistic 3D sign language avatar sequences from text, significantly improving the authenticity and semantic accuracy of sign language generation.

- **Unsupervised sign language translation and generation**[arXiv 2024] [[paper]](https://arxiv.org/abs/2402.07726)  
  **Authors:** Zhengsheng Guo, Zhiwei He, Wenxiang Jiao, Xing Wang, Rui Wang, Kehai Chen, Zhaopeng Tu, Yong Xu, Min Zhang  
  **Introduction:** This paper proposes an unsupervised sign language translation and generation model USLNet, which realizes sign language video and text generation without parallel data through unimodal reconstruction and cross-modal back translation.

- **Sign-Mamba: Advanced Mamba-Based Sign Language Generation**[ICASSP 2025] [[paper]](https://ieeexplore.ieee.org/document/10890373) [[code]](https://peterfanfan.github.io/Sign-Mamba/)  
  **Authors:** Guanwen Feng, Yilin Zhang, Yunan Li, An Liu, Qiguang Miao  
  **Introduction:** This paper proposes Sign-Mamba, a two-stage sign language generation framework based on the Mamba state-space model, which generates high-precision sign language skeleton sequences through latent space encoding and prediction.

- **3DSignDiff: Towards 3D Sign Language Gesture Generation**[ICASSP 2025] [[paper]](https://www.researchgate.net/publication/390538952_3DSignDiff_Towards_3D_Sign_Language_Gesture_Generation)  
  **Authors:** Ronghao Yu, Yun Liu, Xiyue Bai, Rui Yang, Yingna Wu  
  **Introduction:** This study proposes a diffusion-based 3D sign language generation method, which achieves realistic and semantically accurate dynamic sign language avatars through SMPL-X skeleton and graph neural network training on a large-scale 4D dataset.

- **Sign-idd: Iconicity disentangled diffusion for sign language production**[AAAI 2025] [[paper]](https://arxiv.org/abs/2412.13609) [[code]](https://github.com/NaVi-start/Sign-IDD?utm_source=catalyzex.com)  
  **Authors:** Shengeng Tang, Jiayi He, Dan Guo, Yanyan Wei, Feng Li, Richang Hong  
  **Introduction:** This paper proposes the Sign-IDD framework, which models joint associations and skeletal details through image disentanglement and attribute controllable diffusion, achieving more natural and accurate sign language gesture generation.

### Cross-Modal Collaboration-Driven Sign Language Generation
- **Toward an example-based machine translation from written text to ASL using virtual agent animation**[arXiv 2012] [[paper]](https://arxiv.org/abs/1203.3023)  
  **Authors:** Mehrez Boulares, Mohamed Jemni  
  **Introduction:** This paper proposes the Text-Adaptive Sign Language Modeling Language (TASML) to generate natural and expressive ASL animations through semantic annotation, emotion detection and genetic algorithms, thereby making up for the shortcomings of existing automatic sign language translation systems in terms of information and expressiveness.

- **sEditor: A prototype for a sign language interfacing system**[THMS 2014] [[paper]](https://ieeexplore.ieee.org/document/6828758)  
  **Authors:** Beifang Yi, Xusheng Wang, Frederick C Harris, Sergiu M Dascalu  
  **Introduction:** This paper proposes a sign language interface system that realizes sign language creation, management and editing through virtual human body and gesture simulation, providing a demonstration for future sign language communication platforms.

- **Arabic text-to-sign (ArTTS) model from automatic SR system**[PCS 2017] [[paper]](https://www.sciencedirect.com/science/article/pii/S1877050917321798)  
  **Authors:** Omar H Al-Barahamtoshy, Hassanin M Al-Barhamtoshy  
  **Introduction:** This paper proposes an Arabic Text to Arabic Sign Language (ArTTS) system that generates sign language through speech recognition and 3D virtual characters.

- **Automated 3D sign language caption generation for video**[UAIS 2020] [[paper]](https://link.springer.com/article/10.1007/s10209-019-00668-9)  
  **Authors:** Nayan Mehta, Suraj Pai, and Sanjay Singh  
  **Introduction:** This paper proposes a video translation system based on 3D cartoon characters and Indian Sign Language subtitles for classroom teaching, which significantly improves the academic performance and vocabulary acquisition rate of hearing-impaired students.

- **A rule based system for bangla voice and text to bangla sign language interpretation**[STI 2020] [[paper]](https://ieeexplore.ieee.org/document/9350468)  
  **Authors:** Muhammad Aminur Rahaman, Md Parvez Hossain, Md Masud Rana, Md Arifur Rahman, Tahmina Akter  
  **Introduction:** This paper proposes a rule-based system to efficiently translate Bengali speech and text into Bengali sign language animation, enabling high-precision communication between non-sign language speakers and sign language users.

 - **ES2ISL: an advancement in speech to sign language translation using 3D avatar animator**[CCECE 2020] [[paper]](https://ieeexplore.ieee.org/document/9255783)  
  **Authors:** Bhavinkumar Devendrabhai Patel, Harshit Balvantrai Patel, Manthan Ashok Khanvilkar, Nidhi Rajendrakumar Patel, Thangarajah Akilan  
  **Introduction:** The ES2ISL system proposed in this paper can quickly and efficiently convert English speech into Indian Sign Language animation, enabling high-precision communication between the hearing-impaired and others. 

- **Voice to Indian Sign Language Conversion for Hearing Impaired People**[SAMRIDDHI 2020] [[paper]](https://www.academia.edu/65341486/Voice_to_Indian_Sign_Language_Conversion_for_Hearing_Impaired_People?uc-sb-sw=7756885)  
  **Authors:** Ashmi Katariya, Vaibhav Rumale, Aishwarya Gholap, Anuprita Dhamale, Ankita Gupta  
  **Introduction:** This project proposes a system that converts speech input into text and applies Indian Sign Language grammar rules to achieve word-level speech-to-sign language translation.

- **Towards automatic speech to sign language generation**[arXiv 2021] [[paper]](https://arxiv.org/abs/2106.12790) [[code]](https://github.com/kapoorparul/Towards-Automatic-Speech-to-SL)  
  **Authors:** Parul Kapoor, Rudrabha Mukhopadhyay, Sindhu B Hegde, Vinay Namboodiri, CV Jawahar  
  **Introduction:** This study proposes the first end-to-end multi-task Transformer model to generate continuous Indian Sign Language video directly from speech, and verifies the effectiveness of the method through a newly collected dataset.

- **An automatic machine translation system for multi-lingual speech to Indian sign language**[MTA 2022] [[paper]](https://link.springer.com/article/10.1007/s11042-021-11706-1)  
  **Authors:** Amandeep Singh Dhanjal, Williamjeet Singh  
  **Introduction:** This study proposes the SISLA system, which automatically translates multilingual speech into Indian Sign Language and presents it through a 3D virtual image, significantly improving communication between the hearing-impaired and others.

- **Indian sign language generation from live audio or text for tamil**[ICACCS 2023] [[paper]](https://ieeexplore.ieee.org/document/10112880)  
  **Authors:** Bandi Rupendra Reddy, Daka Chandra Rup, Mathi Rohith, Meena Belwal  
  **Introduction:** This paper proposes a model for converting Tamil speech or text into Indian Sign Language animation.
  
- **MultiFacet: a multi-tasking framework for speech-to-sign language generation**[ICMI 2023] [[paper]](https://dl.acm.org/doi/10.1145/3610661.3616550)  
  **Authors:** Mounika Kanakanti, Shantanu Singh, Manish Shrivastava  
  **Introduction:** This paper proposes a multi-task model that combines speech prosody, text semantics, and facial action unit prediction to generate Indian Sign Language gesture sequences.

- **Real-time Arabic avatar for deaf-mute communication enabled by deep learning sign language translation**[CEE 2024] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0045790624004026)  
  **Authors:** Fatma M Talaat, Walid El-Shafai, Naglaa F Soliman, Abeer D Algarni, Fathi E Abd El-Samie, Ali I Siam  
  **Introduction:** This paper proposes a real-time Arabic sign language avatar system based on deep learning, which enables smooth real-time communication by translating text or speech into avatar movements.

- **Interactive Sign Language Learning System using Deep Learning**[ICSSAI 2024] [[paper]](https://ieeexplore.ieee.org/document/10760668)  
  **Authors:** S Ravikumar, P Dimple Praharsha, J Sri Harsha Priya, V Naga Rohit Sai  
  **Introduction:** This paper proposes an interactive sign language learning system that provides learners with a high-precision and comprehensive sign language learning experience through sign language recognition and text-to-motion conversion functions, combined with multilingual support and voice output, and using image processing, machine learning, and natural language processing technologies.
  
- **Real-Time Sign Language and Audio Conversion Using AI**[CCIS 2024] [[paper]](https://ieeexplore.ieee.org/document/10931891)  
  **Authors:** A Boobal, Chennama Charan Kesava Reddy, Challa Akshay Reddy, Chaluvadi Bala Venkata Sai Rohith, et al.  
  **Introduction:** This paper introduces a system that uses artificial intelligence, computer vision, and natural language processing technologies to convert audio into sign language and gesture signals in real time to enhance the communication capabilities of the hearing-impaired and blind.

### Semantic Control-Driven Sign Language Generation
- **An automatic rule-based translation system to Spanish Sign Language (LSE)**[NTHCI 2009] [[paper]](https://link.springer.com/chapter/10.1007/978-1-84882-352-5_1)  
  **Authors:** Sandra Baldassarri, Francisco Royo-Santas  
  **Introduction:** This paper introduces a system for automatic translation of Spanish into Spanish Sign Language (LSE), which achieves versatile and high-quality 3D animation output through morphosyntactic analysis, grammatical conversion, and sign language generation modules.

- **Arabic text to Arabic sign language translation system for the deaf and hearing-impaired community**[SLPAT 2011] [[paper]](https://aclanthology.org/W11-2311/)  
  **Authors:** Abdulaziz Almohimeed, Mike Wald, Robert I Damper  
  **Introduction:** This paper proposes a machine translation system for translating text into Arabic Sign Language (ArSL), enabling access to sign language information in the field of education by developing a corpus in collaboration with deaf native speakers.

- **A rule-based translation from written Spanish to Spanish Sign Language glosses**[CSL 2014] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0885230813000867)  
  **Authors:** Jordi Porta, Fernando López-Colino, Javier Tejedor, José Colás  
  **Introduction:** This paper proposes a rule-based machine translation system for Spanish to Spanish Sign Language (LSE) annotations, enabling sign language translation through dependency parsing and a transfer architecture.

- **A rule triggering system for automatic text-to-sign translation**[UAIS 2016] [[paper]](https://link.springer.com/article/10.1007/s10209-015-0413-4)  
  **Authors:** Michael Filhol, Mohamed N Hadjadj, Benoît Testu  
  **Introduction:** This paper proposes a rule-based machine translation system from French text to French Sign Language (LSF). By triggering generation rules through text processing, it realizes sign language grammar generation and explores the application of machine translation in sign language processing.

- **KAZOO: a sign language generation platform based on production rules**[UAIS 2016] [[paper]](https://link.springer.com/article/10.1007/s10209-015-0415-2)  
  **Authors:** Annelies Braffort, Michael Filhol, Maxime Delorme, Laurence Bolot, Annick Choisier, Cyril Verrecchia  
  **Introduction:** This paper introduces KAZOO, a web application based on a virtual signer, which generates sign language animations through SL corpus analysis and the AZee language model, achieving automatic content synthesis and verification.

- **A frame-based approach for capturing semantics from Arabic text for text-to-sign language MT**[IJST 2016] [[paper]](https://link.springer.com/article/10.1007/s10772-015-9290-8)  
  **Authors:** Abdelaziz Lakhfif, Mohamed Tayeb Laskri  
  **Introduction:** This paper proposes an Arabic semantic parser based on Interlingua to transform Arabic text into deep semantic representations to support Arabic-to-sign language machine translation and natural language processing applications.

- **Automatic translation of Arabic text-to-Arabic sign language**[UAIS 2019] [[paper]](https://link.springer.com/article/10.1007/s10209-018-0622-8)  
  **Authors:** Hamzah Luqman, Sabri A Mahmoud  
  **Introduction:** This paper proposes a rule-based machine translation system to convert Arabic text into Arabic Sign Language (ArSL) sentences and provides a parallel corpus of 600 sentences in the health field to support research.
  
- **ATLASLang MTS 1: Arabic text language into Arabic sign language machine translation system**[PCS 2019] [[paper]](https://www.sciencedirect.com/science/article/pii/S1877050919300699)  
  **Authors:** Mourad Brour, Abderrahim Benabbou  
  **Introduction:** This paper proposes the ATLASLang MTS 1 system, which uses morphosyntactic analysis and a gesture database to translate Arabic text into Arabic sign language videos displayed in real time by a 3D virtual avatar.

- **Sign language generation system based on Indian sign language grammar**[TALLIP 2020] [[paper]](https://dl.acm.org/doi/10.1145/3384202)  
  **Authors:** Sugandhi, Parteek Kumar, Sanmeet Kaur  
  **Introduction:** This paper implements a system that automatically translates English text into Indian Sign Language (ISL) and displays it through a 3D avatar, enabling efficient, human-computer interactive communication without artificial sign language.

- **A novel natural language processing (NLP)-based machine translation model for English to Pakistan sign language translation**[CC 2020] [[paper]](https://link.springer.com/article/10.1007/s12559-020-09731-7)  
  **Authors:** Nabeel Sabir Khan, Adnan Abid, Kamran Abid  
  **Introduction:** This paper highlights the importance of developing effective translation models for Pakistani Sign Language (PSL) speakers to bridge the communication barrier between deaf and hearing individuals and improve information access.

- **Machine translation from spoken language to sign language using pre-trained language model as encoder**[LREC 2020] [[paper]](https://aclanthology.org/2020.signlang-1.23/)  
  **Authors:** Taro Miyazaki, Yusuke Morita, Masanori Sano  
  **Introduction:** This paper proposes a machine translation method based on an encoder-decoder model combined with post-translation replacement of proper nouns, which can effectively improve the translation quality from Japanese to Japanese Sign Language.

- **Translating Spanish into Spanish Sign Language: Combining rules and data-driven approaches**[WTLRMT 2022] [[paper]](https://aclanthology.org/2022.loresmt-1.10/)  
  **Authors:** Luis Chiruzzo, Euan McGill, Santiago Egea-Gómez, and Horacio Saggion  
  **Introduction:** This paper significantly improves the performance of neural machine translation from spoken to sign language by leveraging language feature enhancement and pre-training and fine-tuning on large-scale aligned Spanish-Spanish sign language synthetic data.  

- **Modeling intensification for sign language generation: a computational approach**[ACL 2022] [[paper]](https://aclanthology.org/2022.findings-acl.228/)  
  **Authors:** Mert Inan, Yang Zhong, Sabit Hassan, Lorna Quandt, and Malihe Alikhani  
  **Introduction:** This paper introduces an enhanced annotation strategy based on sign language linguistics and constructs an enhanced dataset, effectively improving the performance of the sign language generation Transformer model in temporal and spatial rhythm, and significantly improving the quality and human preference of the generated videos.

- **Gloss semantic-enhanced network with online back-translation for sign language production**[ACM MM 2022] [[paper]](https://dl.acm.org/doi/10.1145/3503161.3547830)  
  **Authors:** Shengeng Tang, Richang Hong, Dan Guo, and Meng Wang  
  **Introduction:** This paper proposes a semantic enhancement network GEN-OBT based on online back translation. By introducing learnable semantic tags and a bidirectional semantic-gesture consistency mechanism, it significantly improves the accuracy and interpretability of the semantic-to-gesture conversion in sign language generation without the need for prior knowledge.

- **Improving Sign Language Production in the Healthcare Domain Using UMLS and Multi-Task Learning**[PLOP 2024] [[paper]](https://aclanthology.org/2024.cl4health-1.1/)  
  **Authors:** Jonathan David Mutal, Raphael Rubino, Pierrette Bouillon, Bastien David, Johanna Gerlach, and Irene Strasly  
  **Introduction:** This paper proposes a multi-task framework that integrates the Unified Medical Language System (UMLS) to generate multi-channel Swiss-French Sign Language output from French written input, significantly improving the accuracy of sign language translation in medical scenarios.

- **Signs as Tokens: An Autoregressive Multilingual Sign Language Generator**[arXiv 2024] [[paper]](https://arxiv.org/abs/2411.17799) [[code]](https://2000zrl.github.io/soke/?utm_source=catalyzex.com)  
  **Authors:** Ronglai Zuo, Rolandos Alexandros Potamias, Evangelos Ververas, Jiankang Deng, and Stefanos Zafeiriou  
  **Introduction:** This paper proposes a multilingual sign language generation model called SOKE. It achieves efficient autoregressive generation of 3D gestures through a decoupled word segmenter and a multi-head decoding mechanism. It also introduces a retrieval enhancement method combined with an external dictionary to significantly improve the accuracy and fluency of text-to-sign language conversion.

- **A Sign Language Digital Human System Combining LLM and 3D Animation Technology**[CCL 2024] [[paper]](https://aclanthology.org/2024.ccl-3.44/)  
  **Authors:** Yang Yang, Ying Zhang, Kaiyu Huang, and Jinan Xu  
  **Introduction:** This paper proposes a sign language translation system that combines a large language model with 3D animation technology. By optimizing prompt word design and data processing, it achieves more natural and accurate translation without manual annotation.

- **A simple baseline for spoken language to sign language translation with 3d avatars**[ECCV 2024] [[paper]](https://arxiv.org/abs/2401.04730) [[code]](https://github.com/FangyunWei/SLRT)  
  **Authors:** Ronglai Zuo, Fangyun Wei, Zenggui Chen, Brian Mak, Jiaolong Yang, and Xin Tong  
  **Introduction:** This paper proposes the Spoken2Sign system, the first system to translate spoken language into 3D sign language. By building an annotated 3D sign language dictionary and training an end-to-end model, it enables the generation of sign language virtual avatars, while simultaneously supporting 3D key point enhancement and multi-view understanding.

- **Exploring the Potential of Generative AI in Song-Signing**[UbiComp 2024] [[paper]](https://dl.acm.org/doi/10.1145/3675094.3678378)  
  **Authors:** YouJin Choi, JaeYoung Moon, Kyung-Joong Kim, and Jin-Hyuk Hong  
  **Introduction:** Through user research, this study proposes a sign language song creation tool that integrates a large language model and generative dance AI, using generative artificial intelligence to automate music analysis, lyrics translation, and dance choreography.

- **Generating Signed Language Instructions in Large-Scale Dialogue Systems**[NAACL-HLT 2024] [[paper]](https://aclanthology.org/2024.naacl-industry.13/)  
  **Authors:** Mert Inan, Katherine Atwell, Anthony Sicilia, Lorna Quandt, and Malihe Alikhani  
  **Introduction:** This paper introduces the first multimodal conversational AI system that integrates American Sign Language instructions. Through sign language translation driven by a large language model and token-based video retrieval technology, combined with community-involved design, it achieves user experience and high-quality content generation comparable to non-sign language systems in teaching scenarios.

- **Signllm: Sign languages production large language models**[arXiv 2024] [[paper]](https://arxiv.org/abs/2405.10718) [[code]](https://signllm.github.io/?utm_source=catalyzex.com)  
  **Authors:** Sen Fang, Lei Wang, Ce Zheng, Yapeng Tian, and Chen Chen  
  **Introduction:** This paper proposes a large multilingual sign language generation model, SignLLM. Through the innovative MLSF and Prompt2LangGloss modes and a reinforcement learning-based training mechanism, it achieves state-of-the-art multilingual sign language generation performance on the Prompt2Sign dataset containing eight sign languages.

- **LLM-based Sign Language Production**[ICMLA 2024] [[paper]](https://ieeexplore.ieee.org/document/10903313)    
  **Authors:** Wellington Silveira, Luca Mendonça, and Rodrigo De Bem  
  **Introduction:** This paper proposes a text-to-sign language generation method based on a large language model, which uses LLM to translate text into gestures and synthesize image sequences.

- **A Spatio-Temporal Representation Learning as an Alternative to Traditional Glosses in Sign Language Translation and Production**[AACL-HLT 2025] [[paper]](https://arxiv.org/abs/2407.02854)  
  **Authors:** Eui Jun Hwang, Sukmin Cho, Huije Lee, Youngwoo Yoon, and Jong C. Park  
  **Introduction:** This paper proposes a universal annotation-level representation, UniGloR, which extracts dense spatiotemporal features from gesture key points through self-supervised learning, overcoming the limitations of traditional annotations in scalability and dynamic details, and providing a more refined and dynamic alternative representation method for sign language translation and generation.

- **Gloss-driven conditional diffusion models for sign language production**[TOMM 2025] [[paper]](https://dl.acm.org/doi/10.1145/3663572)  
  **Authors:** Shengeng Tang, Feng Xue, Jingjing Wu, Shuo Wang, and Richang Hong  
  **Introduction:** This paper proposes a semantic-driven conditional diffusion model (GCDM), which uses the encoded semantic sequence as a priori condition to guide the denoising process and achieves high-quality generation from text to sign language gesture sequences.

- **Linguistics-Vision Monotonic Consistent Network for Sign Language Production**[ICASSP 2025] [[paper]](https://arxiv.org/abs/2412.16944)  
  **Authors:** Xu Wang, Shengeng Tang, Peipei Song, Shuo Wang, Dan Guo, and Richang Hong  
  **Introduction:** This paper proposes a Transformer-based Language-Vision Monotonic Consistency Network (LVMCN), which achieves the joint optimization of fine-grained action alignment and coarse-grained semantic consistency through a cross-modal semantic aligner and a multimodal semantic comparator.

- **Beyond Words: AuralLLM and SignMST-C for Precise Sign Language Production and Bidirectional Accessibility**[arXiv 2025] [[paper]](https://arxiv.org/abs/2501.00765)    
  **Authors:** Yulong Li, Yuxuan Zhang, Feilong Tang, Mian Zhou, Zhixiang Lu, Haochen Xue, Yifang Wang, Kang Dang, and Jionglong Su  
  **Introduction:** This paper introduces the first unified dataset CNSign/CNText2Sign that supports bidirectional translation of Chinese Sign Language, and proposes the AuraLLM and SignMST-C models, which respectively achieve high-precision sign language generation and direct posture evaluation through decoupled architecture and posture adjustment, and improve translation performance through self-supervised pre-training.

- **Towards AI-driven Sign Language Generation with Non-manual Markers**[CHI 2025] [[paper]](https://arxiv.org/abs/2502.05661)    
  **Authors:** Han Zhang, Rotem Shalev-Arkushin, Vasileios Baltatzis, Connor Gillis, Gierad Laput, Raja Kushalnagar, Lorna C. Quandt, Leah Findlater, Abdelkareem Bedri, and Colin Lea  
  **Introduction:** This paper combines large language models with video generation technology to jointly model the manual and non-manual features of sign language to generate natural ASL videos that are grammatically accurate and rich in facial expressions and body language.  

## Dataset 
- **RWTH-BOSTON-50** [[dataset]](https://www-i6.informatik.rwth-aachen.de/aslr/database-rwth-boston-50.php) [[paper]](https://link.springer.com/chapter/10.1007/11550518_50)    
  **Authors:** Morteza Zahedi, Daniel Keysers, Thomas Deselaers, and Hermann Ney  
  **Introduction:** RWTH-BOSTON-50 was recorded by three signers in a controlled environment with two perspectives and covers 483 segments of 50 common ASL words.

- **Purdue RVL-SLLL ASL** [[dataset]](https://engineering.purdue.edu/RVL/Database/ASL/asl-database-front.htm) [[paper]](https://docs.lib.purdue.edu/ecetr/338/)    
  **Authors:** Ronnie Wilbur and Avinash C. Kak  
  **Introduction:** The RVL-SLLL ASL Database was collected by Purdue University and contains hand shape, finger spelling, digit, word movement, and short text recorded by 14 deaf signers under controlled and uncontrolled lighting. It is used for ASL automatic recognition research.

- **RWTH-BOSTON-104** [[dataset]](https://www-i6.informatik.rwth-aachen.de/aslr/database-rwth-boston-104.php) [[paper]](https://aclanthology.org/L08-1469/)    
  **Authors:** Philippe Dreuw, Carol Neidle, Vassilis Athitsos, Stanley Sclaroff, and Hermann Ney  
  **Introduction:** Based on the Boston University ASL sentence library, this video contains 201 sentences recorded by three signers, shot from multiple perspectives and annotated with vocabulary and language models.

- **SIGNUM** [[dataset]](https://www.phonetik.uni-muenchen.de/forschung/Bas/SIGNUM/) [[paper]](https://www.sign-lang.uni-hamburg.de/lrec/pub/10006.pdf)    
  **Authors:** Ulrich von Agris, Karl-Friedrich Kraiss  
  **Introduction:** TThis dataset, created by the RWTH Aachen University in Germany, uses a single camera to record German sign language continuous recognition data. It aims to support research on large-vocabulary, signer-independent automatic sign language recognition.

- **RWTH-BOSTON-400** [[dataset]](https://www-i6.informatik.rwth-aachen.de/web/Software/Databases/Signlanguage/?db=rwth-boston-104) [[paper]](https://aclanthology.org/L08-1469/)    
  **Authors:** Philippe Dreuw, Carol Neidle, Vassilis Athitsos, Stanley Sclaroff, and Hermann Ney  
  **Introduction:** Based on the Boston University ASL sentence library, this dataset contains approximately 400 sentences from five signers, recorded from multiple perspectives, and annotated with vocabulary and language models for continuous sign language recognition research.

- **Corpus NGT** [[dataset]](https://archive.mpi.nl/tla/islandora/object/tla:1839_00_0000_0000_0004_DF8E_6?asOfDateTime=2018-03-02T11:00:00.000Z) [[paper]](https://pure.mpg.de/rest/items/item_1187628/component/file_1189592/content)    
  **Authors:** Onno A. Crasborn and IEP Zwitserlood  
  **Introduction:** A corpus of Dutch Sign Language, consisting of 2,375 conversation videos annotated with ELAN, publicly available for download.

- **RWTH-PHOENIX-Weather** [[dataset]](https://www-i6.informatik.rwth-aachen.de/web/Software/Databases/Signlanguage/details/rwth-phoenix/index.php) [[paper]](https://aclanthology.org/L12-1503/)    
  **Authors:** Jens Forster, Christoph Schmidt, Thomas Hoyoux, Oscar Koller, Uwe Zelle, Justus H. Piater, and Hermann Ney  
  **Introduction:** A large-vocabulary video corpus of German sign language, based on weather forecast recordings, with gesture variant annotations and spoken alignments, for statistical sign language recognition and translation research.

- **ASLG-PC12** [[dataset]](https://achrafothman.net/site/english-asl-gloss-parallel-corpus-2012-aslg-pc12/) [[paper]](https://www.sign-lang.uni-hamburg.de/lrec/pub/12019.pdf)    
  **Authors:** Achraf Othman and Mohamed Jemni  
  **Introduction:** A large English-American Sign Language parallel corpus containing over 100 million sentence pairs, freely available online for research in sign language processing and statistical machine translation.
  
- **Dicta-Sign** [[dataset]](https://www.sign-lang.uni-hamburg.de/dicta-sign/portal/) [[paper]](https://www.sign-lang.uni-hamburg.de/lrec/pub/12016.pdf)    
  **Authors:** Silke Matthes, Thomas Hanke, Anja Regen, Jakob Storz, Satu Worseck, Eleni Efthimiou, Athanasia-Lida Dimou, Annelies Braffort, John Glauert, and Eva Safar  
  **Introduction:** Contains semi-spontaneous corpora of four European sign languages, annotated with iLex, for research in sign language recognition, translation, language modeling, and generation.

- **BSL Corpus** [[dataset]](https://bslcorpusproject.org/projects/) [[paper]](https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/97f6a096-0e1d-4872-b922-0e72ace5e905/content)    
  **Authors:** Adam Schembri, Jordan Fenlon, Ramas Rentelis, Sally Reynolds, and Kearsy Cormier  
  **Introduction:** A British Sign Language video corpus covering lexical, syntactic, and non-manual features, continuously annotated and translated for use in sign language research, teaching, and development of automatic translation systems.

- **S-pot** [[dataset]](https://research.cs.aalto.fi/cbir/data/s-pot/) [[paper]](https://aclanthology.org/L14-1377/)    
  **Authors:** Ville Viitaniemi, Tommi Jantunen, Leena Savolainen, Matti Karppa, and Jorma Laaksonen  
  **Introduction:** Finnish Sign Language Continuous Recognition Benchmark, consisting of 5539 videos, ground-level annotations, and evaluation tools for evaluating automatic sign language recognition performance.

- **CUNY ASL Motion-Capture** [[dataset]](https://www.sciencedirect.com/science/article/abs/pii/S0885230813000879) [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0885230813000879)    
  **Authors:** Pengfei Lu and Matt Huenerfauth  
  **Introduction:** An American Sign Language motion capture corpus recording detailed hand and body movements for generating and evaluating understandable ASL animations.

- **LSA16** [[dataset]](https://facundoq.github.io/datasets/lsa16/) [[paper]](https://journal.info.unlp.edu.ar/JCST/article/view/518)    
  **Authors:** Franco Ronchetti, Facundo Quiroga, César Armando Estrebou, and Laura Cristina Lanzarini  
  **Introduction:** Contains 800 images of 16 hand shapes recorded by 10 non-signers in an indoor environment, for hand shape recognition research.

- **LSE-Sign** [[dataset]](http://lse-sign.bcbl.eu/web-busqueda/) [[paper]](https://link.springer.com/article/10.3758/s13428-014-0560-1)    
  **Authors:** Eva Gutierrez-Sigut, Brendan Costello, Cristina Baus, and Manuel Carreiras  
  **Introduction:** An online Spanish Sign Language database containing 2,400 signed and 2,700 non-signed items with detailed grammatical, motor, and non-gestural information for experimental material selection.
  
- **ASLA-Leap** [[dataset]](https://github.com/WenjinTao/ASLA-Leap) [[paper]](https://www.mdpi.com/1424-8220/18/10/3554)    
  **Authors:** Wenjin Tao, Ze-Hao Lai, Ming C. Leu, and Zhaozheng Yin  
  **Introduction:** The American Sign Language alphabet dataset, collected using the Leap Motion controller, contains 26 signs performed by 5 subjects, with 500 samples per sign.

- **MS-ASL** [[dataset]](https://www.microsoft.com/en-us/research/project/ms-asl/downloads/) [[paper]](https://arxiv.org/abs/1812.01053)    
  **Authors:** Hamid Reza Vaezi Joze and Oscar Koller  
  **Introduction:** Contains 25,000+ annotated videos, 1000 categories, and 200+ signers, for sign language recognition research and model evaluation in unconstrained environments.

- **RWTH-PHOENIX-2014T** [[dataset]](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/) [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Camgoz_Neural_Sign_Language_CVPR_2018_paper.pdf)    
  **Authors:** Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden  
  **Introduction:** A dataset for continuous German sign language recognition, containing 386 weather program videos, sign language gloss annotations, and spoken language alignments, for multi-signer and signer-independent recognition research.

- **Deep JSLC** [[dataset]](https://aclanthology.org/L18-1670/) [[paper]](https://aclanthology.org/L18-1670/)    
  **Authors:** Heike Brock and Kazuhiro Nakadai  
  **Introduction:** Japanese Sign Language Continuous Recognition Dataset, containing annotated videos for training and evaluating sign language recognition models.

- **KETI** [[dataset]](https://arxiv.org/abs/1811.11436) [[paper]](https://arxiv.org/abs/1811.11436)    
  **Authors:** Sang-Ki Ko, Chang Jo Kim, Hyedong Jung, and Choongsang Cho  
  **Introduction:** Korean Sign Language dataset, containing 14,672 high-quality videos, for research on human gesture sign language translation based on human key points.

- **ASL-100-RGBD** [[dataset]](https://nyu.databrary.org/volume/1062) [[paper]](https://arxiv.org/abs/1906.02851)    
  **Authors:** Longlong Jing, Elahe Vahdani, Matt Huenerfauth, and Yingli Tian  
  **Introduction:** The American Sign Language RGB-D video dataset contains 42 videos with 100 gestures each, with full skeleton, facial, and depth annotations for multimodal sign language recognition research.

- **ChicagoFSWild** [[dataset]](https://home.ttic.edu/~klivescu/ChicagoFSWild.htm#download) [[paper]](https://arxiv.org/abs/1908.10546)    
  **Authors:** Bowen Shi, Aurora Martinez Del Rio, Jonathan Keane, Diane Brentari, Greg Shakhnarovich, and Karen Livescu  
  **Introduction:** A dataset of American Sign Language fingerspelling in the wild, containing 7304 sequences recorded and annotated by 160–260 signers.
  
- **ChicagoFSWild+** [[dataset]](https://home.ttic.edu/~klivescu/ChicagoFSWild.htm#download) [[paper]](https://arxiv.org/abs/1908.10546)    
  **Authors:** Bowen Shi, Aurora Martinez Del Rio, Jonathan Keane, Diane Brentari, Greg Shakhnarovich, and Karen Livescu  
  **Introduction:** A dataset of American Sign Language fingerspelling in the wild, containing 55232 sequences recorded and annotated by 160–260 signers.

- **INCLUDE** [[dataset]](https://www.kaggle.com/datasets/yuvrajjoshi1110/include-50/data) [[paper]](https://dl.acm.org/doi/10.1145/3394171.3413528)    
  **Authors:** Advaith Sridhar, Rohith Gandhi Ganesan, Pratyush Kumar, and Mitesh Khapra  
  **Introduction:** Indian Sign Language dataset, containing 4,287 videos, 263 sign words, and covering 15 word classes, for sign language recognition research and model evaluation.

- **BSL-1K** [[dataset]](https://www.robots.ox.ac.uk/~vgg/research/bsl1k/) [[paper]](https://arxiv.org/abs/2007.12131)    
  **Authors:** Samuel Albanie, Gül Varol, Liliane Momeni, Triantafyllos Afouras, Joon Son Chung, Neil Fox, and Andrew Zisserman  
  **Introduction:** A large-scale British Sign Language dataset containing 1,000 hours of video and a 1,000-sign vocabulary for continuous gesture recognition and sign language translation research.

- **AUTSL** [[dataset]](https://cvml.ankara.edu.tr/datasets/) [[paper]](https://arxiv.org/abs/2008.00932)    
  **Authors:** Ozge Mercanoglu Sincan and Hacer Yalim Keles  
  **Introduction:** A large-scale multimodal Turkish Sign Language dataset containing 226 gestures performed by 43 signers, 38,336 video samples, and RGB, depth, and skeleton information for sign language recognition and benchmark evaluation.

- **WLASL** [[dataset]](https://www.kaggle.com/datasets/risangbaskoro/wlasl-processed) [[paper]](https://arxiv.org/abs/1910.11006)    
  **Authors:** Dongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong Li  
  **Introduction:** A large-scale word-level American Sign Language video dataset containing over 2,000 words and over 100 signers, for word-level sign language recognition and model benchmarking.

- **K-RSL** [[dataset]](https://krslproject.github.io/krsl20/) [[paper]](https://aclanthology.org/2020.conll-1.51.pdf)    
  **Authors:** Alfarabi Imashev, Medet Mukushev, Vadim Kimmelman, and Anara Sandygulova  
  **Introduction:** The Kazakh-Russian Sign Language dataset contains 5,200 videos recording 20 common gestures and their non-manual components, which is used to study the impact of non-manual features on sign language recognition.

- **Dicta-Sign-LSF-v2** [[dataset]](https://www.sign-lang.uni-hamburg.de/dicta-sign/portal/) [[paper]](https://aclanthology.org/2020.lrec-1.740/)    
  **Authors:** Valentin Belissen, Annelies Braffort, and Michèle Gouiffès  
  **Introduction:** French Sign Language dialogue corpus, containing 11 hours of video and 35,000 gesture units from 16 signers, with both lexical and non-lexical annotations, for use in sign language comprehension and recognition research.

- **CSL-Daily** [[dataset]](https://ustc-slr.github.io/datasets/2021_csl_daily/) [[paper]](https://arxiv.org/abs/2105.12397)    
  **Authors:** Hao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, and Houqiang Li  
  **Introduction:** A large-scale continuous Chinese sign language translation dataset covering daily life topics, providing spoken translations and annotation-level annotations for end-to-end SLT research.

- **Content4All** [[dataset]](https://www.cvssp.org/data/c4a-news-corpus/) [[paper]](https://arxiv.org/abs/2105.02351)    
  **Authors:** Necati Cihan Camgöz, Ben Saunders, Guillaume Rochette, Marco Giovanelli, Giacomo Inches, Robin Nachtrab-Ribback, and Richard Bowden  
  **Introduction:** The dataset contains six data sets totaling 190 hours of news footage, 20 hours of which are annotated by deaf experts. It provides sign language video and subtitle alignment and baseline translation for research and development of practical sign language applications.

- **How2Sign** [[dataset]](https://how2sign.github.io/) [[paper]](https://arxiv.org/abs/2008.08143)    
  **Authors:** Amanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti Ghadiyaram, Kenneth DeHaan, Florian Metze, Jordi Torres, and Xavier Giro-i-Nieto  
  **Introduction:** A multimodal, multi-view continuous American Sign Language (ASL) dataset containing over 80 hours of sign language video with corresponding speech, text, and depth information, and a subset of 3D poses for sign language recognition, translation, and generation research.

- **LIBRAS-UFOP** [[dataset]](https://www.kaggle.com/datasets/andersonls/libras-ufop-dataset/data) [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417420309143)    
  **Authors:** Lourdes Ramirez Cerna, Edwin Escobedo Cardenas, Dayse Garcia Miranda, David Menotti, and Guillermo Camara-Chavez  
  **Introduction:** A public multimodal dataset containing 56 highly similar sign languages, providing complete RGB-D and skeleton information, and verified by sign language experts for sign language recognition research.

- **LSFB-CONT** [[dataset]](https://lsfb.info.unamur.be/#dataset) [[paper]](https://ieeexplore.ieee.org/document/9534336/)    
  **Authors:** Jérôme Fink, Benoît Frénay, Laurence Meurant, and Anthony Cleve  
  **Introduction:** A large-scale Catalan Sign Language dataset suitable for continuous and isolated sign recognition, with benchmark experiments to evaluate the performance of deep learning models.

- **LSFB-ISOL** [[dataset]](https://lsfb.info.unamur.be/#dataset) [[paper]](https://ieeexplore.ieee.org/document/9534336/)    
  **Authors:** Jérôme Fink, Benoît Frénay, Laurence Meurant, and Anthony Cleve  
  **Introduction:** A subset of LSFB-CONT suitable for isolated SLR.

- **KArSL** [[dataset]](https://hamzah-luqman.github.io/KArSL/) [[paper]](https://dl.acm.org/doi/10.1145/3423420)    
  **Authors:** Ala Addin I. Sidig, Hamzah Luqman, Sabri Mahmoud, and Mohamed Mohandes  
  **Introduction:** Contains 502 Arabic sign language gestures, repeated 50 times each by three professional signers and recorded using a multimodal Kinect V2, for sign language recognition research and benchmarking.

- **ASL-Homework-RGBD** [[dataset]](https://databrary.org/volume/1249) [[paper]](https://aclanthology.org/2022.signlang-1.11/)    
  **Authors:** Saad Hassan, Matthew Seita, Larwan Berke, Yingli Tian, Elaine Gale, Sooyeon Lee, and Matt Huenerfauth  
  **Introduction:** Contains videos of 45 fluent and non-fluent ASL speakers recorded with Kinect v2, annotated with grammatical features and non-manual markers, for studying ASL fluency and developing sign language recognition algorithms.

- **CISLR** [[dataset]](https://huggingface.co/datasets/Exploration-Lab/CISLR) [[paper]](https://aclanthology.org/2022.emnlp-main.707/)    
  **Authors:** Abhinav Joshi, Ashwani Bhat, Priya Gole, Shashwat Gupta, Shreyansh Agarwal, Ashutosh Modi, et al.  
  **Introduction:** Contains videos of approximately 4,700 Indian Sign Language words for word-level recognition and provides a prototype-based one-shot learning baseline model.    

- **LSA64** [[dataset]](http://facundoq.github.io/datasets/lsa64/) [[paper]](https://arxiv.org/abs/2310.17429)    
  **Authors:** Franco Ronchetti, Facundo Manuel Quiroga, César Estrebou, Laura Lanzarini, and Alejandro Rosete  
  **Introduction:** Contains 3,200 Argentinian Sign Language gesture videos recorded by 10 subjects, covering a total of 64 gestures, for sign language recognition and related machine learning research.
    
- **ASL Citizen** [[dataset]](https://www.microsoft.com/en-us/research/project/asl-citizen/) [[paper]](https://arxiv.org/abs/2304.05934)    
  **Authors:** Aashaka Desai, Lauren Berger, Fyodor Minakov, Nessa Milano, Chinmay Singh, Kriston Pumphrey, Richard Ladner, Hal Daumé III, Alex X. Lu, Naomi Caselli, et al.  
  **Introduction:** Contains 83,399 American Sign Language videos recorded by 52 signers in various settings, covering 2,731 sign languages, for use in isolated sign language recognition and sign language dictionary retrieval research.

- **PopSign ASL v1.0** [[dataset]](https://www.kaggle.com/datasets/mrgeislinger/popsign-asl-v1-0-game-test) [[paper]](https://openreview.net/forum?id=yEf8NSqTPu)    
  **Authors:** Thad Starner, Sean Forbes, Matthew So, David Martin, Rohit Sridhar, Gururaj Deshpande, Sam Sepah, Sahir Shahryar, Khushi Bhardwaj, Tyler Kwok, et al.  
  **Introduction:** A collection of 250 isolated ASL signs captured by 47 American Sign Language users using smartphones, totaling over 210,000 examples, was used for sign language recognition and educational game development.

- **Slovo** [[dataset]](https://www.kaggle.com/datasets/kapitanov/slovo) [[paper]](https://arxiv.org/abs/2305.14527)    
  **Authors:** Alexander Kapitanov, Kvanchiani Karina, Alexander Nagaev, and Elizaveta Petrova  
  **Introduction:** Contains 20,000 high-definition video samples recorded by 194 Russian sign language users, covering 1,000 RSL gesture categories, for sign language recognition research and model training.

- **ArabSign** [[dataset]](https://hamzah-luqman.github.io/ArabSign/) [[paper]](https://arxiv.org/abs/2210.03951)    
  **Authors:** Hamzah Luqman  
  **Introduction:** Contains 9,335 continuous Arabic sign sentence samples (approximately 10 hours) recorded by 6 sign language users, providing RGB, depth, and skeleton data for continuous ArSL recognition research.

- **BDSL 49** [[dataset]](https://www.kaggle.com/datasets/umongsain/bdsl-49) [[paper]](https://www.sciencedirect.com/science/article/pii/S235234092300447X)    
  **Authors:** Ayman Hasib, Jannatul Ferdous Eva, Saqib Sizan Khan, Mst Nipa Khatun, Ashraful Haque, Nishat Shahrin, Rashik Rahman, Hasan Murad, Md Rajibul Islam, and Molla Rashied Hussein  
  **Introduction:** Contains 29,490 images of Bengali sign language letters from 49 categories, taken by 14 adults, for sign language recognition and detection research.  

- **YouTube-ASL** [[dataset]](https://github.com/google-research/google-research/tree/master/youtube_asl) [[paper]](https://arxiv.org/abs/2306.15162)    
  **Authors:** Dave Uthus, Garrett Tanzer, and Manfred Georg  
  **Introduction:** A large-scale open-domain ASL video dataset containing approximately 1,000 hours of video and over 2,500 individual signers, for sign language recognition and translation research.

- **SASL** [[dataset]](https://ieeexplore.ieee.org/document/10293328) [[paper]](https://ieeexplore.ieee.org/document/10293328)    
  **Authors:** Mokgadi Setshekgamollo, Mohohlo Tsoeu, and Robyn Verrinder  
  **Introduction:** The first parallel corpus of South African Sign Language and English, containing 5047 sentences, for use in vision-based neural sign language translation research.

- **SASL** [[dataset]](https://ieeexplore.ieee.org/document/10293328) [[paper]](https://ieeexplore.ieee.org/document/10293328)    
  **Authors:** Mokgadi Setshekgamollo, Mohohlo Tsoeu, and Robyn Verrinder  
  **Introduction:** The first parallel corpus of South African Sign Language and English, containing 5047 sentences, for use in vision-based neural sign language translation research.
