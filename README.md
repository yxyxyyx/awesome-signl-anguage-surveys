# awesome-signl-anguage-surveys
This repository complements our survey on developments in sign language technology with a curated list of papers investigating sign language processing, including recognition, translation, and generation.

# Menu
- [Surveys](#surveys)
- [Papers](#papers)

# Surveys
## Sign language recognition
- **Automatic Sign Language Analysis: A Survey and the Future beyond Lexical Meaning**[TPAMI 2005] [[paper]](https://www.researchgate.net/publication/7799837_Automatic_Sign_Language_Analysis_A_Survey_and_the_Future_beyond_Lexical_Meaning)  
  **Authors:** Sylvie Ong, Surendra Ranganath  
  **Introduction:** This paper summarizes the research progress in the field of automatic sign language analysis, focusing on continuous sign language gesture recognition, non-gesture signal analysis and its combination with gestures, and proposes future research directions.

- **Image-based and sensor-based approaches to Arabic sign language recognition**[THMS 2014] [[paper]](https://ieeexplore.ieee.org/abstract/document/6814287)  
  **Authors:** Mohamed Mohandes, Mohamed Deriche, Junzhao Liu  
  **Introduction:** This paper reviews the methods and systems for automatic Arabic sign language recognition, analyzes the main challenges, and proposes future research directions.

- **Wearable sensor-based sign language recognition: A comprehensive review**[RBME 2020] [[paper]](https://ieeexplore.ieee.org/abstract/document/9178440)  
  **Authors:** Karly Kudrinko, Emile Flavin, Xiaodan Zhu, Qingguo Li  
  **Introduction:** This paper reviews the research on sign language recognition based on wearable sensors, reviews 72 studies from 1991–2019, analyzes trends, methods, and challenges, and provides a reference for the development of user-centered sign language recognition systems.

 - **A Comprehensive Study on Deep Learning-Based Methods for Sign Language Recognition**[TMM 2021] [[paper]](https://ieeexplore.ieee.org/abstract/document/9393618)  
  **Authors:** Nikolas Adaloglou, Theocharis Chatzis, Ilias Papastratis, Andreas Stergioulas, Georgios Th. Papadopoulos, Vassia Zacharopoulou, George J. Xydopoulos, Klimnis Atzakas, Dimitris Papazachariou, Petros Daras  
  **Introduction:** This paper reviews and evaluates deep learning-based sign language recognition methods, proposes a new sequence training criterion, discusses pre-training schemes, and creates the first multi-level annotation-level RGB+D dataset of Greek sign language.

- **Sign language recognition: A deep survey**[ESA 2021] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S095741742030614X)  
  **Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera  
  **Introduction:** This paper reviews the deep learning-based visual sign language recognition models developed in the past five years, and analyzes the classification, datasets, applications, challenges, and future research directions of isolated and continuous sign language recognition.

- **Artificial intelligence technologies for sign language**[Sensors 2021] [[paper]](https://www.mdpi.com/1424-8220/21/17/5843)  
  **Authors:** Ilias Papastratis, Christos Chatzikonstantinou, Dimitrios Konstantinidis, Kosmas Dimitropoulos, Petros Daras  
  **Introduction:** This paper reviews the deep learning-based visual sign language recognition models developed in the past five years, and analyzes the classification, datasets, applications, challenges, and future research directions of isolated and continuous sign language recognition.

- **A Survey of Sign Language Recognition, Translation, and Generation**[CS (China) 2021] [[paper]](https://qikan.cqvip.com/Qikan/Article/Detail?id=7103984847&from=Qikan_Article_Detail)  
  **Authors:** Dan Guo, Shengeng Tang, Richang Hong, Meng Wang  
  **Introduction:** This paper reviews the latest progress, typical methods and challenges in sign language recognition, translation and generation, and looks forward to the future development direction of this field.

- **Sign language recognition systems: A decade systematic literature review**[ACME 2021] [[paper]](https://link.springer.com/article/10.1007/s11831-019-09384-2)  
  **Authors:** Dan Guo, Shengeng Tang, Richang Hong, Meng Wang  
  **Introduction:** This paper reviews the latest progress, typical methods and challenges in sign language recognition, translation and generation, and looks forward to the future development direction of this field.

- **Emerging wearable interfaces and algorithms for hand gesture recognition: A survey**[RBME 2021] [[paper]](https://ieeexplore.ieee.org/abstract/document/9426433)  
  **Authors:** Shuo Jiang, Peiqi Kang, Xinyu Song, Benny PL Lo, Peter B Shull  
  **Introduction:** This paper reviews the applications, method classification and challenges of wearable gesture interfaces and algorithms in gesture recognition, and proposes future research directions to improve accuracy and practicality.

- **A survey on sign language literature**[MLwA 2023] [[paper]](https://www.sciencedirect.com/science/article/pii/S2666827023000579)  
  **Authors:** Marie Alaghband, Hamid Reza Maghroor, Ivan Garibay  
  **Introduction:** This article reviews the research progress of sign language recognition and translation, covering methods, datasets, and applications, to provide a reference for bridging the communication gap between the hearing-impaired and the hearing-normal.

- **Unraveling a decade: A comprehensive survey on isolated sign language recognition**[CVPR 2023] [[paper]](https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Sarhan_Unraveling_a_Decade_A_Comprehensive_Survey_on_Isolated_Sign_Language_ICCVW_2023_paper.html)  
  **Authors:** Noha Sarhan, Simone Frintrop  
  **Introduction:** This paper reviews the latest methods, datasets, and performance of isolated sign language recognition (ISLR), and analyzes the research progress, challenges, and future development directions.

- **A Review of Sign Language Systems**[DeSE 2023] [[paper]](https://www.researchgate.net/publication/379160316_A_Review_of_Sign_Language_Systems)  
  **Authors:** Marzieh Moradi, Deepika Dhanabalan Kannan, Shiva Asadianfam, Hoshang Kolivand, Omar Aldhaibani  
  **Introduction:** Provides a comprehensive review of sign language systems, with particular attention to research on British Sign Language (BSL), and examines the legal, social, and ethical considerations associated with the use of sign languages.

- **Understanding Sign Language Recognition: An Overview**[ICEECT 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10739257)  
  **Authors:** Shivani Saini, Meenu Prjapati, Bhupal Arya, Ravinder Kumar Sharma, Manoj Kumar  
  **Introduction:** This paper reviews the progress of artificial intelligence technologies in the field of sign language recognition (SLR), covering both vision-based and sensory glove-based methods, aiming to help eliminate communication barriers for the deaf and mute.

- **A review of modular continuous sign language recognition algorithms and technologies**[Mini-Micro S. 2024] [[paper]](https://qikan.cqvip.com/Qikan/Article/Detail?id=7113060967&from=Qikan_Search_Index)  
  **Authors:** Meng Jinkai, Peng Jianjun, Xiao Zhidong, Guo Li, Jin Kai, Zheng Tong  
  **Introduction:** This paper reviews the research progress of continuous sign language recognition, focusing on the keyframe extraction, feature extraction and sequence learning modules in unimodal and multimodal frameworks, summarizes the datasets and evaluation metrics, and discusses the challenges of existing algorithms and future development directions.

- **A Survey of Methods and Technologies for Chinese Sign Language Recognition**[Mod. Spec. Educ. 2024] [[paper]](https://qikan.cqvip.com/Qikan/Article/Detail?id=7111887169&from=Qikan_Search_Index)  
  **Authors:** Meng Jinkai, Peng Jianjun, Xiao Zhidong, Guo Li, Jin Kai, Zheng Tong  
  **Introduction:** This paper reviews the development of Chinese sign language recognition from traditional methods to modern deep learning and artificial intelligence technologies, which provides important support for the communication between the hearing-impaired and the society.
   
- **Sign Language Recognition and Translation Systems for Enhanced Communication for the Hearing Impaired**[2024 1st IC-CGU] [[paper]](https://ieeexplore.ieee.org/abstract/document/10530832)  
  **Authors:** Kambhampati Sai Sindhu, Nikitha Biradar, Penumathsa Likhita Varma, Chandrasekhar Uddagiri, et al.  
  **Introduction:** This paper reviews the technical progress and challenges of sign language recognition and translation systems, emphasizing the importance of their grammatical complexity and diverse data requirements for developing efficient and inclusive communication tools.

- **Literature Review: Recognization of Sign Language and Translation Systems**[INSPECT 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10896036)  
  **Authors:** P. Jyothi, Rishitha Sagar Mogili, Raman Garg, Tanmayee Etla, R. Kundan  
  **Introduction:** This paper reviews the research progress of sign language recognition and translation, and analyzes key methods, technologies, and models to promote barrier-free communication between sign language users and non-sign language users.

- **Deep Learning Method for Sign Language Recognition: A Systematic Literature Review**[2024 ICIMTech] [[paper]](https://ieeexplore.ieee.org/abstract/document/10780830)  
  **Authors:** Sekar Ayu Nadita, Lavender Nathania Adelya, Daniel Hendra Susanto, Gusti Pangestu  
  **Introduction:** This paper systematically reviews the application of artificial intelligence (especially deep learning) in sign language recognition, evaluates the performance of different models, techniques and datasets, points out the challenges in research, and demonstrates the recognition accuracy of up to 99.98% and the potential for future development.

- **A review of real-time sign language recognition for virtual interaction on meeting platforms**[Confluence 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10463439)  
  **Authors:** Mohd Faisal, Angad Singh, Shailendra Narayan Singh  
  **Introduction:** This study reviews the current status, challenges, and technical methods of real-time sign language recognition and translation systems in virtual meetings, providing a reference for improving multilingual sign language-assisted communication.

- **Exploring Modern Approaches of Recognizing Sign Language: A Review**[ICAC²N 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10895826)  
  **Authors:** Rani Astya, Kalpana Mishra, Anil Kumar Sagar  
  **Introduction:** This paper reviews the latest machine learning methods and their applications for sign language recognition, analyzes the current technical advantages and challenges, and points out future research directions to improve system performance and usability.

- **A Survey on Machine and Deep Learning Approaches in Sign Language Recognition: Techniques and Future Trends**[IConSCEPT 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10627910)  
  **Authors:** C. Harshitha, Narendran Rajagopalan, et al.  
  **Introduction:** This review summarizes the progress in the application of machine learning and deep learning in sign language recognition and explores future research directions.

- **Advancements in sign language recognition: A comprehensive review and future prospects**[Access 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10670380)  
  **Authors:** Bashaer Al Abdullah, Ghada Amoudi, Hanan Alghamdi  
  **Introduction:** This paper systematically reviews the latest progress of automatic sign language translation systems (SLTS), emphasizes the role of deep learning in improving sign language recognition accuracy and integrating non-artificial features, and proposes future research directions to improve the applicability and performance of the systems.

- **VR Applications and Vision-Based Recognition Techniques for Sign Language: A Survey**[CICN 2024] [[paper]](https://ieeexplore.ieee.org/document/10847543)  
  **Authors:** Sumayyah Seher, Kulsoom Abedi, Kulsum Fatima, Sadiya Badiwalla, Taha Houda  
  **Introduction:** This article reviews the combination of virtual reality and visual sign language recognition, emphasizing its application potential and future development directions in immersive education and barrier-free sign language learning.

- **Sign Language or Gesture-Based Recognition System: A Review**[ASSIC 2024] [[paper]](https://ieeexplore.ieee.org/document/10507916)  
  **Authors:** Anjana Mishra, Ayush Kumar, Abhinav Kumar, Abhishek Mahato, Keshav Kamal  
  **Introduction:** This paper reviews the latest methods and challenges of sign language recognition using computer vision, machine learning, and Internet of Things technologies to facilitate efficient communication between deaf and mute people and others.

- **A review of deep learning-based approaches to sign language processing**[AR 2024] [[paper]](https://www.tandfonline.com/doi/full/10.1080/01691864.2024.2442721?src=)  
  **Authors:** Sihan Tan, Nabeela Khan, Zhaoyi An, Yoshitaka Ando, Rei Kawakami, Kazuhiro Nakadai  
  **Introduction:** This review systematically reviews the development of sign language recognition, translation, and generation technologies, focusing on the application of large language models, the applicability of datasets, and the limitations of existing methods, providing a reference for promoting standardized evaluation and the development of robust sign language processing systems.

- **Sign language recognition: A comprehensive review of traditional and deep learning approaches, datasets, and challenges**[Access 2024] [[paper]](https://ieeexplore.ieee.org/document/10526274)  
  **Authors:** Tangfei Tao, Yizhe Zhao, Tianyu Liu, Jieli Zhu  
  **Introduction:** This paper reviews the recent developments in sign language recognition (SLR), covering traditional and deep learning methods, datasets, challenges, and future directions, aiming to provide a reference for sign language understanding and research.

- **A Meta-Analytic Quantitative Review of Methods and Techniques for Sign Language Recognition Approaches**[IST-Africa 2024] [[paper]](https://ieeexplore.ieee.org/document/10569745)  
  **Authors:** Tebatso Gorgina Moape, Absolom Muzambi, Bester Chimbo  
  **Introduction:** This paper summarizes the overall effects and trends of machine learning and deep learning methods in the field of sign language recognition through meta-analysis, reveals significant heterogeneity among studies and fills the gap in systematic review.

- **A Survey of Dynamic Sign Language Recognition**[CSA 2025] [[paper]](https://www.c-s-a.org.cn/html/2025/5/9879.htm)  
  **Authors:** Wang Zhekai, Feng Yunxia, Wang Jiawen  
  **Introduction:** This paper reviews the current development status, methods, datasets and evaluation indicators of dynamic sign language recognition technology based on deep learning, analyzes the challenges and shortcomings of existing methods, and looks forward to future research directions.

- **Deep learning pathways for automatic sign language processing**[PR 2025] [[paper]](https://www.sciencedirect.com/science/article/pii/S0031320325001359)  
  **Authors:** Mukhiddin Toshpulatov, Wookey Lee, Jaesung Jun, Suan Lee  
  **Introduction:** This paper reviews the research progress in recognition, translation, generation and datasets in the field of sign language processing, analyzes key technologies and challenges, and proposes future development directions.

- **A Survey of Sign Language Recognition and Translation Based on Computer Vision**[M&C 2025] [[paper]](https://lib.cqvip.com/Qikan/Article/Detail?id=7201183340&from=Qikan_Search_Index)  
  **Authors:** Yunan Li, Xi Geng, Qiguang Miao  
  **Introduction:** This paper systematically reviews computer vision-based sign language recognition and translation methods, datasets, and evaluation metrics, analyzes their respective characteristics and challenges, and proposes future research directions.

- **Deep learning approaches for continuous sign language recognition: A comprehensive review**[Access 2025] [[paper]](https://ieeexplore.ieee.org/document/10937713)  
  **Authors:** Asma Khan, Seyong Jin, Geon-Hee Lee, Gul E Arzu, Tan N Nguyen, L Minh Dang, Woong Choi, Hyeonjoon Moon  
  **Introduction:** This paper reviews deep learning methods in the field of continuous sign language recognition (CSLR), proposes a unified classification framework, analyzes the advantages and disadvantages of spatial, temporal, and alignment strategies, and discusses key challenges such as dataset diversity, real-time processing, and differences among sign language users, providing guidance for future research.
   
## Sign language translation
- **Automatic Sign Language Analysis: A Survey and the Future beyond Lexical Meaning**[22nd NETA 2018] [[paper]](https://d.wanfangdata.com.cn/conference/10402801)  
  **Authors:** Ankita Wadhawan, Parteek Kumar  
  **Introduction:** This paper systematically reviews and categorizes sign language recognition research from 2007 to 2017, analyzing methods, gesture types, and performance to provide reference and guidance for future research.

- **Advances in machine translation for sign language: approaches, limitations, and challenges**[NCA 2021] [[paper]](https://link.springer.com/article/10.1007/s00521-021-06079-3)
        
        
        
        
  **Authors:** Uzma Farooq, Mohd Shafry Mohd Rahim, Nabeel Sabir, Amir Hussain, Adnan Abid  
  **Introduction:** This article reviews the latest research progress in sign language interpretation from a multidisciplinary perspective, classifies and analyzes methods, compares theoretical foundations, and explores challenges and future development directions.

- **Deep learning methods for sign language translation**[TACCESS 2021] [[paper]](https://dl.acm.org/doi/abs/10.1145/3477498)
        
        
        
        
  **Authors:** Tejaswini Ananthanarayana, Priyanshu Srivastava, Akash Chintha, Akhil Santha, Brian Landy, Joseph Panaro, Andre Webster, Nikunj Kotecha, Shagan Sah, Thomastine Sarchet, et al.  
  **Introduction:** This paper reviews and evaluates sign language translation methods based on deep learning, compares the performance of different input features and neural translation models on various sign language datasets, and points out that Transformer combined with ResNet50 or posture features performs best.

- **A survey of advancements in real-time sign language translators: integration with IoT technology**[Technologies 2023] [[paper]](https://www.mdpi.com/2227-7080/11/4/83)  
  **Authors:** Maria Papatsimouli, Panos Sarigiannidis, George F. Fragulis  
  **Introduction:** This article reviews the development of real-time sign language interpretation systems and their integration with Internet of Things technologies over the past five years, analyzing technological progress, current applications, and future potential to promote communication and inclusion for the deaf and hard-of-hearing community.

- **Sign language translation: A survey of approaches and techniques**[Electronics 2023] [[paper]](https://www.mdpi.com/2079-9292/12/12/2678)  
  **Authors:** Zeyu Liang, Huailing Li, Jianping Chai  
  **Introduction:** This paper reviews the research progress of sign language translation (SLT), including the basic model, Transformer framework, four types of subtasks, challenges and future directions.

- **A survey on Sign Language machine translation**[ESA 2023] [[paper]](https://www.sciencedirect.com/science/article/pii/S0957417422020115)  
  **Authors:** Adrián Núñez-Marcos, Olatz Perez-de-Viñaspre, Gorka Labaka  
  **Introduction:** This study systematically reviews the development of sign language translation (SLT), introduces the background and mainstream datasets, and summarizes the challenges and future research directions.

- **Machine translation from text to sign language: a systematic review**[UAIS 2023] [[paper]](https://link.springer.com/article/10.1007/s10209-021-00823-1)
        
        
        
        
  **Authors:** Navroz Kaur Kahlon, Williamjeet Singh  
  **Introduction:** This paper reviews the research progress, method classification, advantages and disadvantages of sign language machine translation and generation, and emphasizes the improvement of automated sign language translation through deep learning and neural networks.

- **Sign Language Recognition and Translation Methods Promote Sign Language Education: A Review**[SMC 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10831194)  
  **Authors:** Jingchen Zou, Jianqiang Li, Jing Tang, Yuning Huang, Shujie Ding, Xi Xu  
  **Introduction:** This paper systematically reviews the development, key technical methods and latest progress of sign language recognition and translation (SLRT), analyzes the limitations of existing methods and proposes future research directions.

- **A Survey on Recognition and Translation System of Real-Time Sign Language**[2024 2nd IDICAIEI] [[paper]](https://ieeexplore.ieee.org/abstract/document/10842738)  
  **Authors:** Kajal Dakhare, Vidhi Wankhede, Prateek Verma  
  **Introduction:** This paper reviews the latest deep learning methods for real-time sign language recognition systems and their applications in improving sign language understanding accuracy and real-time interaction, providing technical support for improving communication for the deaf and mute.

- **Deep Learning Methods of Dynamic Sign Language Translation: A Rapid Umbrella Review**[ICECET 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10698113)  
  **Authors:** Lebogang Puree Bopape, Mohohlo Samuel Tsoeu  
  **Introduction:** This paper presents the first systematic review of existing sign language translation and recognition, analyzing technical methods, geographical background, and linguistic characteristics. It also points out that existing research tends to be data-driven and lacks attention to the complexity of sign language and reasonable evaluation.

- **Review of Intelligent Methods for Sign Language-to-Speech Translation**[ICALTER 2024] [[paper]](https://ieeexplore.ieee.org/abstract/document/10819213)  
  **Authors:** Edita Chafloque Cajusol Trujillo, Luis Felipe Castillo, Juri Aquino  
  **Introduction:** This paper reviews CNN- and RNN-based sign language-to-speech intelligent systems, analyzes their technical progress and challenges, and emphasizes their potential in enhancing the social inclusion and autonomy of the deaf and mute.

- **Deep Learning--Based Sign Language Translation: Past, Present, and Future**[ARC 2024] [[paper]](https://www.arocmag.cn/abs/2025.01.0001)  
  **Authors:** Lei Zhang, Zhenyu Wang, Shuaishuai Lian, Bingqian Pu, Yutao Liu, Mingzhe Qin  
  **Introduction:** This paper reviews deep learning-based sign language translation (SLT) methods, analyzes the characteristics and performance of different model categories, and explores future development directions such as real-time translation and large-scale model fine-tuning.

## Sign language production
- **Sign language production: A review**[CVPR 2021] [[paper]](https://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Rastgoo_Sign_Language_Production_A_Review_CVPRW_2021_paper.html)  
  **Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera, Mohammad Sabokrou  
  **Introduction:** This paper reviews the latest progress of deep learning in the field of sign language generation, and analyzes its advantages, limitations and future research directions.

- **A Survey on Neural Machine Translation Applied to Sign Language Generation**[ICAML 2021] [[paper]](https://ieeexplore.ieee.org/abstract/document/9712072)  
  **Authors:** Yue Zhang, Lihong Cao  
  **Introduction:** This paper reviews sign language generation methods based on neural machine translation, including RNN and Transformer models, datasets and evaluation methods, and proposes future research directions.

- **State of the art of automation in Sign Language: a systematic review**[TALLIP 2023] [[paper]](https://dl.acm.org/doi/abs/10.1145/3564769)  
  **Authors:** Rakesh Kumar Attar, Vishal Goyal, Lalit Goyal  
  **Introduction:** This paper systematically reviews the research progress, method classification and comparative analysis of sign language generation systems from 1998 to 2020, and proposes future research directions to guide the development of this field.

- **Development Status and Trends of Sign Language Digital Humans Based on Intelligent Generative Technologies**[AI 2023] [[paper]](https://qikan.cqvip.com/Qikan/Article/Detail?id=7110318684)  
  **Authors:** Shengeng Tang, Xueyu Xiu, Dan Guo, Richang Hong  
  **Introduction:** This paper reviews the current development status, challenges and future trends of sign language digital humans based on intelligent generation technology.

  - **Techniques for Generating Sign Language a Comprehensive Review**[JIEI:B 2024] [[paper]](https://link.springer.com/article/10.1007/s40031-024-01118-8)  
  **Authors:** Prachi Pramod Waghmare  
  **Introduction:** This article reviews the applications of deep learning and natural language processing techniques in sign language generation, highlighting the progress in automatically converting written or spoken language into sign language gestures, the importance of datasets and context, as well as the challenges and opportunities of real-time generation, diverse sign language variants, and ethical issues.

- **A survey on recent advances in Sign Language Production**[ESA 2024] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417423033481)  
  **Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera, Vassilis Athitsos, Mohammad Sabokrou  
  **Introduction:** This paper reviews the progress of sign language recognition and generation (SLP) technology based on deep learning, and discusses the key components, challenges, methodological framework and future research directions of bidirectional sign language translation systems.

# Papers
## Perception Modeling
- **Segmentation of the face and hands in sign language video sequences using color and motion cues**[TCSVT 2004] [[paper]](https://ieeexplore.ieee.org/document/1318645)  
  **Authors:** Nariman Habili, Cheng Chew Lim, Alireza Moini  
  **Introduction:** This paper proposes a hand-face segmentation method based on color and motion information. Through skin color segmentation, motion change detection and segmentation mask generation, it can achieve accurate positioning of hands and faces in sign language videos.

- **Accurate and Accessible Motion-Capture Glove Calibration for Sign Language Data Collection**[TACCESS 2010] [[paper]](https://dl.acm.org/doi/abs/10.1145/1838562.1838564)  
  **Authors:** Matt Huenerfauth, Pengfei Lu  
  **Introduction:** This paper proposes an efficient calibration scheme for motion capture gloves, which significantly improves calibration accuracy and ease of use. Experimental verification shows that it can better support American Sign Language gesture recording and animation generation.

- **Novel FPGA implementation of hand sign recognition system with SOM--Hebb classifier**[TCSVT 2014] [[paper]](https://ieeexplore.ieee.org/document/6848809)  
  **Authors:** Hiroomi Hikawa, Keishi Kaida  
  **Introduction:** This paper proposes an FPGA hardware gesture recognition system based on a SOM-Hebbian hybrid network to achieve efficient and robust recognition of 24 American Sign Language gestures, suitable for embedded applications.

- **Feature extraction in Brazilian Sign Language Recognition based on phonological structure and using RGB-D sensors**[ESA 2014] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417414003042)  
  **Authors:** Sí­lvia Grasiella Moreira Almeida, Frederico Gadelha Guimarães, Jaime Arturo Ramírez  
  **Introduction:** This paper proposes a feature extraction and classification method for Brazilian Sign Language (BSL) based on RGB-D features and the phonological structure of sign language. SVM is used to achieve gesture recognition with an average accuracy of over 80%.

- **Glove-based continuous Arabic sign language recognition in user-dependent mode**[THMS 2015] [[paper]](https://ieeexplore.ieee.org/document/7061411)  
  **Authors:** Noor Tubaiz, Tamer Shanableh, Khaled Assaleh  
  **Introduction:** This paper proposes a glove-based Arabic sign language recognition system that uses sensors to capture gestures and classifies them through an improved k-nearest neighbor method, achieving a sentence recognition rate of up to 98.9%.

- **Isolated sign language recognition with grassmann covariance matrices**[TACCESS 2016] [[paper]](https://dl.acm.org/doi/10.1145/2897735)  
  **Authors:** Hanjie Wang, Xiujuan Chai, Xiaopeng Hong, Guoying Zhao, Xilin Chen  
  **Introduction:** This paper proposes a multimodal sign language representation method based on the Grassmann covariance matrix, which implements discriminative learning through SVM. It outperforms existing methods on three datasets and achieves high accuracy and low computational cost.

- **A feature covariance matrix with serial particle filter for isolated sign language recognition**[ESA 2016] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417416300112)  
  **Authors:** Kian Ming Lim, Alan W.C. Tan, Shing Chiang Tan  
  **Introduction:** This paper proposes an isolated sign language recognition method based on feature covariance matrix and serial particle filtering. By fusing median and mode filtering, hand detection and tracking are achieved, laying the foundation for compact sign language representation and intelligent translation systems.

- **Hand sign language recognition using multi-view hand skeleton**[ESA 2020] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417420301615)  
  **Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera  
  **Introduction:** This paper proposes a multimodal pipeline combining SSD, 2DCNN, 3DCNN and LSTM for efficient sign language gesture recognition from RGB videos and constructing hand skeletons and discriminative spatiotemporal features.

- **ASL trigger recognition in mixed activity/signing sequences for RF sensor-based user interfaces**[THMS 2021] [[paper]](https://ieeexplore.ieee.org/document/9660776)  
  **Authors:** Emre Kurtoğlu, Ali C. Gurbuz, Evie A. Malaia, Darrin Griffin, Chris Crawford, Sevgi Z. Gurbuz  
  **Introduction:** This paper proposes a gesture recognition method based on radio frequency sensors. By using multi-domain data representation, it can distinguish trigger gestures from gross movements, achieving high accuracy rates of 98.9% and 92% for ASL words and gross movements, respectively.

- **Smart, comfortable wearable system for recognizing Arabic Sign Language in real-time using IMUs and features-based fusion**[ESA 2021] [[paper]](https://ieeexplore.ieee.org/document/9660776)  
  **Authors:** Aziz Qaroush, Sara Yassin, Ali Al-Nubani, Ameer Alqam  
  **Introduction:** This paper proposes an Arabic sign language recognition system based on a multi-IMU sensor glove. By fusing accelerometer and gyroscope features, it realizes letter-level gesture recognition, achieving user-dependent and independent recognition accuracies of 98.6% and 96%, respectively.
  
- **Double handed dynamic Turkish Sign Language recognition using Leap Motion with meta learning approach**[ESA 2023] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417423009557)  
  **Authors:** Zekeriya Katılmiş, Cihan Karakuzu  
  **Introduction:** This paper studies bimanual dynamic word recognition in Turkish Sign Language using the LMC device, achieving high-performance and stable word recognition through three-stage feature processing and Meta-ELM classifier.

- **Comparison between handcraft feature extraction and methods based on Recurrent Neural Network models for gesture recognition by instrumented gloves: A case for Brazilian Sign Language Alphabet**[BSPC 2023] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S1746809422006553)  
  **Authors:** Thiago Simões Dias, José Jair Alves Mendes Junior, Sérgio Francisco Pichorim  
  **Introduction:** This paper reviews sensor- and deep learning-based Brazilian Sign Language (Libras) recognition methods and proposes an integrated GRU model to classify signals collected by an instrumented glove to compare the performance with and without manual feature extraction methods.

- **Static hand gesture recognition in sign language based on convolutional neural network with feature extraction method using ORB descriptor and Gabor filter**[ESA 2023] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417422016256)  
  **Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera  
  **Introduction:** This paper proposes a deep learning sign language static gesture recognition network that combines CNN, Gabor filter and ORB features to achieve high accuracy and enhance robustness to uncertainties such as rotation and blur.

- **Spatial-temporal feature-based End-to-end Fourier network for 3D sign language recognition**[ESA 2024] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417424001234)  
  **Authors:** Sunusi Bala Abdullahi, Kosin Chamnongthai, Veronica Bolon-Canedo, Brais Cancela  
  **Introduction:** This paper proposes a dynamic sign language word recognition method based on multi-scale spatiotemporal features (FS-EFCNN), which effectively improves the recognition accuracy of different sign language datasets through low-cost feature selection and Fourier convolutional neural network.
  
## Sign language recognition
### Statistical model-driven temporal modeling
- **Visual recognition of american sign language using hidden markov models**[MIT 1995] [[paper]](https://www.researchgate.net/publication/33835956_Visual_Recognition_of_American_Sign_Language_Using_Hidden_Markov_Models)  
  **Authors:** Starner, Thad  
  **Introduction:** This paper proposes a sentence-level American Sign Language recognition system based on Hidden Markov Models, which achieves 99.2% word accuracy without explicitly modeling fingers.

- **Real-time american sign language recognition from video using hidden markov models**[ISCV 1995] [[paper]](https://ieeexplore.ieee.org/document/477012)  
  **Authors:** Starner, Thad; Pentland, Alex  
  **Introduction:** This paper proposes a real-time sentence-level American Sign Language recognition system based on Hidden Markov Models, which achieves high-accuracy word recognition without explicitly modeling fingers.

- **Large vocabulary sign language recognition based on fuzzy decision trees**[Trans. S.M.C. 2004] [[paper]](https://ieeexplore.ieee.org/document/1288342)  
  **Authors:** Gaolin Fang, Wen Gao, Debin Zhao  
  **Introduction:** This paper proposes a fuzzy decision tree method based on heterogeneous classifiers, which realizes large-vocabulary sign language recognition through hierarchical screening and SOFM/HMM classifiers, significantly reducing recognition time and improving accuracy.

- **American sign language (ASL) recognition based on Hough transform and neural networks**[ESA 2007] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417405003040)  
  **Authors:** Munib Qutaishat, Habeeb Moussa, Takruri Bayan, Al-Malik Hiba Abed  
  **Introduction:** This paper proposes a bare-hand ASL static gesture recognition system based on Hough transform and neural network, which can recognize letters and symbols in a natural way without gloves or markers.

- **Sign transition modeling and a scalable solution to continuous sign language recognition for real-world applications**[TACCESS 2016] [[paper]](https://dl.acm.org/doi/10.1145/2850421)  
  **Authors:** Li, Kehuang, Zhou, Zhengyu, Lee, Chin-Hui  
  **Introduction:** A continuous sign language recognition method based on hidden Markov modeling is proposed. By collecting data through low-cost gloves, robust modeling of gestures and their conversions is achieved. With a vocabulary of 510 words, the word accuracy rate reaches 87.4% and supports real-time recognition.

### Data-driven spatiotemporal deep learning
- **Persian sign language (PSL) recognition using wavelet transform and neural networks**[ESA 2011] [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417410008523)  
  **Authors:** Karami, Ali, Zanj, Bahman, Sarkaleh, Azadeh Kiani  
  **Introduction:** This paper proposes a Persian sign language static letter recognition system based on wavelet transform and multi-layer perceptron neural network, which achieves an average classification accuracy of 94.06% on bare hand images.

- **Sign language recognition using convolutional neural networks**[ECCV 2015] [[paper]](https://link.springer.com/chapter/10.1007/978-3-319-16178-5_40)  
  **Authors:** Pigou, Lionel, Dieleman, Sander, Kindermans, Pieter-Jan, Schrauwen, Benjamin  
  **Introduction:** This paper proposes an automatic sign language recognition system based on Kinect, CNN, and GPU acceleration, which achieves high-accuracy recognition of 20 Italian gestures and generalizes to unseen users and environments.

- **Attention-based 3D-CNNs for large-vocabulary sign language recognition**[TCSVT 2018] [[paper]]](https://ieeexplore.ieee.org/document/8466903)  
  **Authors:** Huang, Jie, Zhou, Wengang, Li, Houqiang, Li, Weiping  
  **Introduction:** This paper proposes an attention-based 3D-CNN framework to automatically extract spatiotemporal features from videos and select key actions for efficient sign language recognition. 

- **A Deep Neural Framework for Continuous Sign Language Recognition by Iterative Training**[TMM 2019] [[paper]]](https://ieeexplore.ieee.org/document/8598757)  
  **Authors:** Cui, Runpeng, Liu, Hu, Zhang, Changshui  
  **Introduction:** This paper proposes an iteratively optimized continuous sign language recognition framework based on deep convolutional neural networks and bidirectional recurrent neural networks, which significantly improves the recognition performance through multimodal fusion.

- **Hand Gesture Recognition for Sign Language Using 3DCNN**[Access 2020] [[paper]]](https://ieeexplore.ieee.org/document/9078786)  
  **Authors:** Al-Hammadi, Muneer, Muhammad, Ghulam, Abdul, Wadood, Alsulaiman, Mansour, Bencherif, Mohamed A., Mekhtiche, Mohamed Amine  
  **Introduction:** This paper proposes a deep convolutional neural network gesture recognition method based on transfer learning, which achieves high-precision recognition on different sign language datasets. 

- **Vision-based hand gesture recognition using deep learning for the interpretation of sign language**[ESA 2021] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S0957417421010484)  
  **Authors:** Sharma, Sakshi, Singh, Sukhwinder  
  **Introduction:** This paper proposes a compact and efficient convolutional neural network model that achieves nearly 100% gesture recognition accuracy on Indian Sign Language and American Sign Language datasets while being robust to rotation and scaling.  

- **ELM based two-handed dynamic turkish sign language (TSL) word recognition**[ESA 2021] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S0957417421006461)  
  **Authors:** Katilmis, Zekeriya, Karakuzu, Cihan  
  **Introduction:** This paper proposes a Turkish Sign Language dynamic vocabulary recognition system based on Leap Motion and feature dimensionality reduction, using the ML-KELM classifier to achieve high accuracy and stable performance.  

- **An optimized generative adversarial network based continuous sign language classification**[ESA 2021] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S0957417421007077)  
  **Authors:** Elakkiya, R, Vijayakumar, Pandi, Kumar, Neeraj  
  **Introduction:** This paper proposes a method based on hyperparameter optimized generative adversarial networks (H-GANs) for efficiently and accurately classifying manual and non-manual gestures of sign language in continuous videos, and improves recognition performance while maintaining low complexity.

- **Sign language recognition based on R (2+ 1) D with spatial-temporal-channel attention**[THMS 2022] [[paper]]](https://ieeexplore.ieee.org/document/9702523)  
  **Authors:** Han, Xiangzu, Lu, Fei, Yin, Jianqin, Tian, Guohui, Liu, Jun  
  **Introduction:** An improved R(2+1)D model is proposed, combined with a lightweight spatiotemporal channel attention module, which effectively separates and enhances the spatial, temporal and channel features in sign language videos, thereby improving sign language recognition performance on multiple datasets.

- **Hear Sign Language: A Real-Time End-to-End Sign Language Recognition System**[TMC 2022] [[paper]]](https://ieeexplore.ieee.org/document/9261114)  
  **Authors:** Wang, Zhibo, Zhao, Tengda, Ma, Jinxin, Chen, Hongkai, Liu, Kaixin, Shao, Huajie, Wang, Qian, Ren, Ju  
  **Introduction:** This paper proposes DeepSLR, an end-to-end real-time sign language recognition system based on an attention mechanism. It uses multi-channel IMU and sEMG sensors to capture arm and finger movements, achieving high-accuracy continuous sign language translation without gesture segmentation.

- **Spatial--temporal enhanced network for continuous sign language recognition**[TCSVT 2023] [[paper]]](https://ieeexplore.ieee.org/document/10185608)  
  **Authors:** Yin, Wenjie, Hou, Yonghong, Guo, Zihui, Liu, Kailin  
  **Introduction:** This paper proposes a spatiotemporal enhancement network that effectively extracts gesture and facial dynamic features through spatial-visual alignment and temporal feature difference modules, thereby improving the performance of continuous sign language recognition.

- **A hybrid approach for Bangla sign language recognition using deep transfer learning model with random forest classifier**[ESA 2023] [[paper]]](https://www.sciencedirect.com/science/article/pii/S0957417422019327)  
  **Authors:** Das, Sunanda, Imtiaz, Md Samir, Neom, Nieb Hasan, Siddique, Nazmul, Wang, Hui  
  **Introduction:** This paper proposes a hybrid model of convolutional neural networks and random forest classifiers that combines deep transfer learning to achieve efficient automatic recognition of Bengali sign language (numbers and letters), and verifies its accuracy and feasibility on a public dataset.

- **Continuous sign language recognition for hearing-impaired consumer communication via self-guidance network**[TCE 2023] [[paper]]](https://ieeexplore.ieee.org/document/10359133)  
  **Authors:** Xue, Wanli, Kang, Ze, Guo, Leming, Yang, Shourui, Yuan, Tiantian, Chen, Shengyong  
  **Introduction:** This paper proposes a self-guided network (SGN) framework to enhance the feature representation of continuous sign language recognition through spatial, temporal and category constraints, significantly improving the recognition performance on multiple CSLR benchmark datasets.

- **Interactive attention and improved GCN for continuous sign language recognition**[BSPC 2023] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S1746809423003646)  
  **Authors:** Guo, Qi, Zhang, Shujun, Tan, Liwei, Fang, Ke, Du, Yinghao  
  **Introduction:** This paper proposes a continuous sign language recognition method based on an interactive attention mechanism and an improved hand movement decoupling GCN. By fusing RGB and skeleton features and introducing a cascaded attention module, efficient spatiotemporal modeling of bimanual movements is achieved, thereby improving CSLR performance.

- **A two-stream sign language recognition network based on keyframe extraction method**[ESA 2024] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S0957417424011345)  
  **Authors:** Liu, Tianyu, Tao, Tangfei, Zhao, Yizhe, Zhu, Jieli  
  **Introduction:** This paper proposes a sign language recognition method that combines keyframe extraction and a full sign language dual-stream network, which effectively summarizes video information and focuses on hand features, achieving excellent performance on multiple sign language datasets.

- **Spatial Temporal Aggregation for Efficient Continuous Sign Language Recognition**[TETCI 2024] [[paper]]](https://ieeexplore.ieee.org/document/10488467)  
  **Authors:** Hu, Lianyu, Gao, Liqing, Liu, Zekang, Liu, Wei  
  **Introduction:** This paper proposes the STAgg method, which effectively reduces the computational and memory overhead of continuous sign language recognition by aggregating similar frames into a unified representation, while maintaining or even improving the recognition accuracy.

- **American Sign language fingerspelling recognition in the wild with spatio temporal feature extraction and multi-task learning**[ESA 2024] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S0957417423034036)  
  **Authors:** Pannattee, Peerawat, Kumwilaisak, Wuttipong, Hansakunbuntheung, Chatchawarn, Thatphithakkul, Nattanun, Kuo, C-C Jay  
  **Introduction:** In this paper, we propose a fingerspelling recognition system that combines VTCNN temporal feature extraction, multi-task learning, supervised contrastive learning, and CTC/attention joint decoding, achieving state-of-the-art performance on the ChicagoFSWild and ChicagoFSWild+ datasets.

- **An ultra-low-computation model for understanding sign languages**[ESA 2024] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S0957417424006481)  
  **Authors:** Fallah, Mohammad K, Najafi, Mohammadreza, Gorgin, Saeid, Lee, Jeong-A  
  **Introduction:** A low-computation sign language recognition method is proposed. Through image abstraction and fully connected neural networks, high-precision recognition (ASL, ISL, BSL) is achieved while significantly reducing model size and computational overhead.

- **Word separation in continuous sign language using isolated signs and post-processing**[ESA 2024] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S095741742400561X)  
  **Authors:** Rastgoo, Razieh, Kiani, Kourosh, Escalera, Sergio  
  **Introduction:** A two-stage model is proposed, which uses CNN+SVD+LSTM trained with isolated gestures and a post-processing algorithm to achieve efficient detection and recognition of isolated gesture boundaries in continuous sign language videos.

- **StepNet: Spatial-temporal part-aware network for isolated sign language recognition**[TOMM 2024] [[paper]]](https://arxiv.org/abs/2212.12857)  
  **Authors:** Shen, Xiaolong, Zheng, Zhedong, Yang, Yi  
  **Introduction:** We propose StepNet, an RGB-based spatiotemporal component-aware network that captures hand and facial features through component-level spatial and temporal modeling, achieving competitive accuracy on multiple sign language recognition benchmarks.

- **MSE-GCN: A Multiscale Spatiotemporal Feature Aggregation Enhanced Efficient Graph Convolutional Network for Dynamic Sign Language Recognition**[TETCI 2024] [[paper]]](https://ieeexplore.ieee.org/document/10799160)  
  **Authors:** Naz, Neelma, Sajid, Hasan, Ali, Sara, Hasan, Osman, Ehsan, Muhammad Khurram  
  **Introduction:** This paper proposes a multi-scale efficient graph convolutional network (MSE-GCN), combined with a spatiotemporal joint attention mechanism, to achieve high-precision and low-computational cost feature extraction and classification in skeleton sign language recognition.

### Multimodal collaborative semantic enhancement
- **Large-Vocabulary Continuous Sign Language Recognition Based on Transition-Movement Models**[ITSMC 2007] [[paper]]](https://ieeexplore.ieee.org/document/4032919)  
  **Authors:** Fang, Gaolin; Gao, Wen; Zhao, Debin  
  **Introduction:** This paper proposes a transition movement model (TMM) method to effectively handle adjacent gesture transitions in large-vocabulary continuous sign languages ​​through dynamic clustering and iterative segmentation.

- **Taiwan sign language (TSL) recognition based on 3D data and neural networks**[ESA 2009] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S0957417407005210)  
  **Authors:** Lee, Yung-Hui, Tsai, Cheng-Yueh  
  **Introduction:** The system based on 3D gesture data and neural network proposed in this paper can accurately recognize Taiwanese Sign Language static gestures with an average accuracy of 96.58% and has good robustness.

- **Discriminative Exemplar Coding for Sign Language Recognition With Kinect**[T-Cybern 2013] [[paper]]](https://ieeexplore.ieee.org/document/6544211)  
  **Authors:** Sun, Chao; Zhang, Tianzhu; Bao, Bing-Kun; Xu, Changsheng; Mei, Tao  
  **Introduction:** This paper proposes a Discriminative Sample Coding (DEC) method to select the most discriminative samples from Kinect videos through multi-instance learning, thereby achieving efficient recognition of sign language videos.

- **Semantic boundary detection with reinforcement learning for continuous sign language recognition**[TCSVT 2020] [[paper]]](https://ieeexplore.ieee.org/document/9106402)  
  **Authors:** Wei, Chengcheng; Zhao, Jian; Zhou, Wengang; Li, Houqiang  
  **Introduction:** This paper proposes a semantic boundary detection method based on reinforcement learning, which accurately aligns video frames and sign language words through multi-scale feature learning to achieve weakly supervised continuous sign language recognition.

- **L-sign: Large-vocabulary sign gestures recognition system**[THMS 2022] [[paper]]](https://ieeexplore.ieee.org/document/9714154)  
  **Authors:** Zheng, Zhiwen; Wang, Qingshan; Yang, Dejun; Wang, Qi; Huang, Wei; Xu, Yinlong  
  **Introduction:** This paper proposes an L-sign system based on a smart bracelet. It uses entropy matching to segment gestures and combines a three-branch CNN with semantic voting to achieve large-vocabulary sign language recognition. The average accuracy of 200 commonly used gestures exceeds 90%.

- **A sign language recognition system with pepper, lightweight-transformer, and LLM**[arXiv 2023] [[paper]]](https://arxiv.org/abs/2309.16898)  
  **Authors:** Lim, JongYoon; Sa, Inkyu; MacDonald, Bruce; Ahn, Ho Seok  
  **Introduction:** This study proposes a method that combines lightweight deep learning with a large language model, enabling the Pepper robot to understand American Sign Language and generate natural gesture and voice responses, thereby achieving efficient and intuitive non-verbal human-computer interaction.

- **The ASL Dataset for Real-Time Recognition and Integration with LLM Services**[IJET 2024] [[paper]]](https://ijet.ise.pw.edu.pl/index.php/ijet/article/view/10.24425-ijet.2024.152513)  
  **Authors:** Chwesiuk, Michał, Popis, Piotr  
  **Introduction:** This study verified the high accuracy of American Sign Language (ASL) recognition using a multi-user gesture image dataset and machine learning methods, demonstrating that individual differences have limited impact on performance, providing new insights for improving the reliability of automatic sign language recognition systems and barrier-free communication technologies.

- **Cross-Modal Adaptive Prototype Learning for Continuous Sign Language Recognition**[T-CSVT 2025] [[paper]]](https://ieeexplore.ieee.org/document/10896751)  
  **Authors:** Wei, Dong; Yang, Xu-Hua; Weng, Yiyang; Lin, Xuanyu; Hu, Hongxiang; Liu, Sheng  
  **Introduction:** This paper proposes CAP-SLR, a continuous sign language recognition method that combines keyframe extraction, multi-scale convolutional attention, and cross-modal adaptive prototype learning. It effectively alleviates the problems of spatiotemporal redundancy and weak supervision, and achieves high-precision recognition on multiple datasets.

- **A structure-based disentangled network with contrastive regularization for sign language recognition**[ESA 2025] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S0957417425002453)  
  **Authors:** Gao, Liqing; Zhu, Lei; Hu, Lianyu; Shi, Peng; Wan, Liang; Feng, We  
  **Introduction:** This paper proposes a structured decoupled network (SD-Net), which improves the accuracy of sign language recognition by separating sentence templates and content entities and fusing their features while utilizing temporal context modeling.
  
- **SCOPE: Sign Language Contextual Processing with Embedding from LLMs**[AAAI 2025] [[paper]]](https://arxiv.org/abs/2409.01073)  
  **Authors:** Liu, Yuqi; Zhang, Wenqian; Ren, Sihan; Huang, Chengyu; Yu, Jingyi; Xu, Lan  
  **Introduction:** This paper proposes SCOPE, a visual sign language recognition and translation framework that combines conversational context and large-scale language models, and releases a new dataset containing 72 hours of Chinese sign language conversations.
      
## Sign language translation
### Rule-based statistical visual pattern mapping
- **Increasing adaptability of a speech into sign language translation system**[ESA 2013] [[paper]]](https://www.sciencedirect.com/science/article/abs/pii/S0957417412010202)  
  **Authors:** López-Ludeña, Verónica; San-Segundo, Rubén; Morcillo, Carlos González; López, Juan Carlos; Muñoz, José M Pardo  
  **Introduction:** This paper proposes an improved speech-to-sign language translation system that achieves rapid adaptation to new tasks or new semantic domains by enhancing the adaptability of speech recognition, machine translation, and 3D animation modules, while maintaining low error rate and high translation quality while reducing adaptation costs.

- **Statistical Machine Translation for Greek to Greek Sign Language Using Parallel Corpora Produced via Rule-Based Machine Translation**[CIMA@ICTAI 2018] [[paper]]](https://www.semanticscholar.org/paper/Statistical-Machine-Translation-for-Greek-to-Greek-Kouremenos-Ntalianis/bfe80a47199e1f5be29fe58eb7a7c9410dffbdd5)  
  **Authors:** Dimitris Kouremenos; Klimis S. Ntalianis; Georgios Siolas; Andreas Stafylopatis  
  **Introduction:** This paper presents a Rule-Based Machine Translation (RBMT) system that enables high-quality Greek text to Greek Sign Language (GSL) glossed corpus creation, facilitating statistical machine translation and overcoming challenges related to GSL's lack of written form and grammar knowledge.
  
- **Rule-based Machine Translation into Ukrainian Sign Language Using Concept Dictionary**[AAAI 2025] [[paper]]](https://arxiv.org/abs/2409.01073)  
  **Authors:** Liu, Yuqi; Zhang, Wenqian; Ren, Sihan; Huang, Chengyu; Yu, Jingyi; Xu, Lan  
  **Introduction:** This paper proposes SCOPE, a visual sign language recognition and translation framework that combines conversational context and large-scale language models, and releases a new dataset containing 72 hours of Chinese sign language conversations.

### Deep learning-based temporal-spatial pattern modeling
  
## Sign language production
